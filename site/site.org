#+OPTIONS: ':nil -:nil ^:{} num:t toc:nil
#+AUTHOR: Hiroyuki Yamada
#+CREATOR: Emacs 26.1 (Org mode 9.1.14 + ox-hugo)
#+HUGO_FRONT_MATTER_FORMAT: toml
#+HUGO_CUSTOM_FRONT_MATTER: :disableToc true
#+HUGO_LEVEL_OFFSET: 1
#+HUGO_SECTION: .
#+HUGO_BASE_DIR: ./
#+HUGO_DATE_FORMAT: %Y-%m-%dT%T+09:00
#+DATE: <2019-02-10 Sun>
#+HUGO_WEIGHT: auto

#+STARTUP: showall logdone

* DONE cpprb
CLOSED: [2021-01-21 Thu 21:22]
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:UNNUMBERED: t
:END:

[[https://ymd_h.gitlab.io/cpprb/][https://ymd_h.gitlab.io/cpprb/images/logo.png]]

#+html: {{% button-center href="/cpprb/installation/" icon="fas fa-download" icon-position="right" %}}Get Started{{% /button-center %}}

---

** Make Reinforcement Learning Easier
:PROPERTIES:
:UNNUMBERED: t
:END:

[[/cpprb/images/replay_buffer.png]]

Experience Replay is widely used for off-policy reinforcement
learning. With cpprb, you can start your experiment quickly without
implementing troublesome replay buffer.

---

** Fast & Flexible
:PROPERTIES:
:UNNUMBERED: t
:END:

#+html: {{< container >}}{{< row >}}{{< col size="md">}}

#+html: {{% markdownify %}}
*** Native
Heavy calculation is implemented with C++ and [[https://cython.org/][Cython]]. cpprb is usually
faster than Python naive implementation.
#+html: {{% /markdownify %}}

#+html: {{< /col >}}{{< col size="md">}}

#+html: {{% markdownify %}}
*** Multiprocessing
cpprb supports [[https://arxiv.org/abs/1803.00933][Ape-X]] on single computer. You don't need to think
problematic lock. cpprb locks only critical section internally well.
#+html: {{% /markdownify %}}

#+html: {{% button-center href="/cpprb/features/ape-x/" icon="fas fa-angle-double-right" icon-position="right" %}}Learn More{{% /button-center %}}

#+html: {{< /col >}}{{< col size="md">}}

#+html: {{% markdownify %}}
*** Flexible Environment
cpprb adopts flexible environment. Any numbers of [[https://numpy.org/][Numpy]] compatible
environment values can be stored.
#+html: {{% /markdownify %}}

#+html: {{% button-center href="/cpprb/features/flexible_environment/" icon="fas fa-angle-double-right" icon-position="right" %}}Learn More{{% /button-center %}}

#+html: {{< /col >}}{{< col size="md">}}

#+html: {{% markdownify %}}
*** Framework Free
You can build your own reinforcement learning algorithms together with
your favorite deep learning library (e.g. [[https://www.tensorflow.org/][TensorFlow]], [[https://pytorch.org/][PyTorch]]).
#+html: {{% /markdownify %}}


#+html: {{< /col >}} {{< /row >}} {{< /container >}}

---

** Ecosystem & Community
:PROPERTIES:
:UNNUMBERED: t
:END:

#+html: {{< container >}} {{< row >}} {{< col size="md">}}

#+html: {{% markdownify %}}
*** Duscussion
Any questions, requests, and so on are welcome.
#+html: {{% /markdownify %}}

#+html: {{% button-center href="https://github.com/ymd-h/cpprb/discussions" icon="fas fa-comments" icon-position="right" %}}User Forum{{% /button-center %}}

#+html: {{< /col >}} {{< col size="md">}}

#+html: {{% markdownify %}}
*** TF2RL
TF2RL provides a set of reinforcement learning algorithms for TensorFlow 2.
TF2RL uses cpprb for off-policy algorithm.
#+html: {{% /markdownify %}}

#+html: {{% button-center href="https://github.com/keiohta/tf2rl" icon="fab fa-github" icon-position="right" %}}TF2RL{{% /button-center %}}

#+html: {{< /col >}} {{< col size="md">}}

#+html: {{% markdownify %}}
*** User Repos
You can find awesome repositories using cpprb. We're looking forward
to seeing your great work will show up.

#+html: {{% /markdownify %}}

#+html: {{% button-center href="https://github.com/ymd-h/cpprb/network/dependents" icon="fab fa-github" icon-position="right" %}}User Repos{{% /button-center %}}

#+html: {{< /col >}} {{< /row >}} {{< /container >}}

* Installation
:PROPERTIES:
:EXPORT_HUGO_SECTION_FRAG: installation
:END:


* Features
:PROPERTIES:
:EXPORT_HUGO_SECTION_FRAG: features
:END:

** DONE Flexible Environment
CLOSED: [2019-11-08 Fri 05:58]
:PROPERTIES:
:EXPORT_FILE_NAME: flexible_environment
:END:

*** Overview

In ~cpprb~ version 8 and newer, you can store any number of
environments (aka. observation, action, etc.).

For example, you can add your special environments like
~next_next_obs~, ~second_reward~, and so on.

These environments can take multi-dimensional shape (e.g. ~3~,
~(4,4)~, ~(84,84,4)~), and any [[https://numpy.org/devdocs/user/basics.types.html][numpy data type]].


**** ~__init__~
In order to construct replay buffers, you need to specify the second
parameter of their constructor, ~env_dict~.

The ~env_dict~ is a ~dict~ whose keys are environment name and whose
values are ~dict~ describing their properties.

The following table is supported properties and their default values.

| key   | description                    | type                         | default value                                    |
|-------+--------------------------------+------------------------------+--------------------------------------------------|
| shape | shape (size of each dimension) | ~int~ or array like of ~int~ | ~1~                                              |
| dtype | data type                      | ~numpy.dtype~                | ~default_dtype~ in constructor or ~numpy.single~ |

**** ~add~
When ~add~ -ing environments to the replay buffer, you have to pass
them by keyword arguments (aka. ~key=value~ style). If your
environment name is not a syntactically valid identifier, you can
still create dictionary first, then unpack the dictionary by ~**~
operator (e.g. ~rb.add(**kwargs)~).

**** ~sample~
~sample~ returns ~dict~ with keys of environments' name and with
values of sampled ones.


*** Example Usage

#+begin_src python
from cpprb import ReplayBuffer
import numpy as np

buffer_size = 32

rb = ReplayBuffer(buffer_size,{"obs": {"shape": (4,4)},
                               "act": {"shape": 1},
                               "rew": {},
                               "next_obs": {"shape": (4,4)},
                               "next_next_obs": {"shape": (4,4)},
                               "done": {},
                               "my_important_info": {"dtype": {np.short}}})

for _ in range(100):
    rb.add(obs=np.zeros((4,4)),
           act=1.5,
           rew=0.0,
           next_obs=np.zeros((4,4)),
           next_next_obs=np.zeros((4,4)),
           done=0,
           my_important_info=2)

rb.sample(64)
#+end_src
*** Notes
~priorities~, ~weights~, and ~indexes~ for ~PrioritizedReplayBuffer~
are special environments and are automatically set.


*** Technical Detail
Internally, these flexible environments are implemented with (cython
version of) ~numpy.ndarray~. They were implemented with C++ code in
older than version 8, which had trouble in flexibilities of data type
and the number of environment. (There was a dirty hack to put all
extra environments into ~act~ which was not treat specially.)


** DONE Multistep-add
CLOSED: [2019-11-10 Sun 14:08]
:PROPERTIES:
:EXPORT_FILE_NAME: multistep_add
:END:

*** Overview
In cpprb, you can add multistep environment to replay buffer simultaneously.


*** Example Usage
#+begin_src python
import numpy as np
from cpprb import ReplayBuffer

rb = ReplayBuffer(32,{"obs": {"shape": 5},
                      "act": {"shape": 3},
                      "rew": {},
                      "next_obs": {"shape": 5},
                      "done": {}})

steps = 10

rb.get_stored_size() # -> 0


rb.add(obs=np.ones(steps,5),
       act=np.zeros(steps,3),
       rew=np.ones(steps),
       next_obs=np.ones(steps,5),
       done=np.zeros(steps))


rb.get_stored_size() # -> steps
#+end_src
*** Notes
The dimension for step must be 0th dimension

*** Technical Detail
The shapes for ~add~ for every environments are stored as
~add_shape=(-1,*env_shape)~ at constructor, s.t. ~env_shape~ is the environment
shape.

Only one environment value is used to determine the step size by
reshaping to ~add_shape~, so that user must pass the values of the
same step size.


** DONE Prioritized Experience Replay
CLOSED: [2019-11-10 Sun 13:26]
:PROPERTIES:
:EXPORT_FILE_NAME: PER
:END:

*** Overview
Prioritized experience replay was proposed by [[https://arxiv.org/abs/1511.05952][T. Schaul et al.]], and
is widely used to speed up reinforcement learning (as far as I know).

Roughly speaking, mis-predicted observations will be learned more
frequently. To compensate distorted probability, weight of learning is
scaled to the opposite direction (cf. importance sampling).

In cpprb, ~PrioritizedReplayBuffer~ class implements these
functionalities with proportional base (instead of rank base)
priorities as shown at bellow table.

#+CAPTION: Parameter Definition
| \(i\)-th                | Definition                                                                                                                                                  |
|-------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Probability: \( P(i) \) | \( \frac{(P_{i}+\epsilon)^{\alpha}}{\sum _{j=0}^{N} (P_{j}+\epsilon)^{\alpha}} \)                                                                           |
| Weight:  \( w_i \)      | \( \frac{\left( \frac{1}{N} \frac{1}{P(i)}\right) ^{\beta}}{\left( \frac{1}{N} \frac{1}{\min _{j} P(j)}\right) ^{\beta}} \to \left(\frac{\min _{j} P(j)}{P(i)}\right) ^{\beta} \) |

You can ~add~ ~priorities~ together with other environment. If no
~priorities~ are passed, the stored maximum priority is used.


The ~dict~ returned by ~sample~ also has special key-values of
~indexes~ and ~weights~. The ~indexes~ are intended to be passed to
~update_priorities~ to update their priorities after comparison with
new prediction. The ~weights~ can be used to scale errors when
updating network parameters. Since the ~weights~ are normalized by the
largest weight, as the original research did, the values are always
less than or equal to 1.


~PrioritizedReplayBuffer~ has hyperparameters ~alpha~ (\( \alpha \))
and ~eps~ (\( \epsilon \)) at constructor and ~beta~ (\( \beta \)) at
~sample~ method. Their default values are ~0.6~, ~1e-4~, and ~0.4~,
respectively. The detail is described in the original paper above.


#+CAPTION: Nstep Parameters
| Parameters             | Default       | Description                                                                   |
|------------------------+---------------+-------------------------------------------------------------------------------|
| ~alpha~ (\( \alpha \)) | \( 0.6 \)     | Prioritized parameter. ~0~ means uniform sampling.                            |
| ~beta~ (\( \beta \))   | \( 0.4 \)     | Compensation parameter. ~1~ means fully compensate (i.e. Importance Sampling) |
| ~eps~ (\( \epsilon \)) | \( 10^{-4} \) | Small value to avoid ~0~ priority.                                            |


*** Example Usage
#+begin_src python
import numpy as np
from cpprb import PrioritizedReplayBuffer

buffer_size = 256

prb = PrioritizedReplayBuffer(buffer_size,
                              {"obs": {"shape": (4,4)},
                               "act": {"shape": 3},
                               "rew": {},
                               "next_obs": {"shape": (4,4)},
                               "done": {}},
                              alpha=0.5)

for i in range(1000):
    prb.add(obs=np.zeros((4,4)),
            act=np.ones(3),
            rew=0.5,
            next_obs=np.zeros((4,4)),
            done=0)

batch_size = 32
s = prb.sample(batch_size,beta=0.5)

indexes = s["indexes"]
weights = s["weights"]

#  train
#  ...

new_priorities = np.ones_like(weights)
prb.update_priorities(indexes,new_priorities)
#+end_src
*** Notes
The constructor of ~PrioritizedReplayBuffer~ (aka. ~__init__~) has
parameter ~check_for_update~. If the parameter is ~True~ (default is
~False~), the buffer traces updated indices after the last calling of
~sample()~ method and the ~update_priorities()~ method updates
priorities only at unchanged indices. This feature is designed for
multiprocess learning in order to avoid mis-updating priorities of
already overwritten values. (This protection is always enabled at
~MPPrioritizedReplayBuffer~)

*** Technical Detail
To choose prioritized sample efficiently, partial summation and
minimum of pre-calculated weights are stored in Segment Tree data
structure, which is written by C++ and which was an initial main
motivation of this project.

To support multiprocessing, the Segment Tree can be lazily updated,
too. (~MPPrioritizedReplayBuffer~)

Minibatch sampling is done by stratified sampling, which samples from
every equal probability space, resulting smaller distributions among
minibatches.


** DONE Nstep Experience Replay
CLOSED: [2019-11-10 Sun 13:46]
:PROPERTIES:
:EXPORT_FILE_NAME: nstep
:END:
*** Overview

To reduce fluctuation of random sampling effect especially at
bootstrap phase, N-step reward (discounted summation) are useful. By
expanding Bellman equation, a N-step target of Q function becomes
$\sum _{k=0}^{N-1} \gamma ^k r_{t+k} + \gamma ^N \max _{a}
Q(s_{t+N},a)$.

According to [[https://arxiv.org/abs/2007.06700][W. Fedus et al.]], N-step reward can utilize larger buffer
more effectively. Even though theoretically N-step reward, which is
based on a policy at exploration, is not justified for off-policy, it
still works better.

You can create N-step version replay buffer by specifying ~Nstep~
parameter at constructors of ~ReplayBuffer~ or
~PrioritizedReplayBuffer~. Without modification of its environment,
cpprb summarizes N-step rewards and slides "next" values like ~next_obs~.

~Nstep~ parameter is a ~dict~ with keys of ~"size"~ , ~"rew"~,
~"gamma"~ , and ~"next"~ . ~Nstep["size"]~ is a N-step size and 1-step
is identical with ordinary replay buffer (but
inefficient). ~Nstep["rew"]~, whose type is ~str~ or array-like of
~str~, specifies the (set of) reward(s). ~Nstep["gamma"]~ is a
discount factor for reward summation.  ~Nstep["next"]~ , whose type is
~str~ or array like of ~str~, specifies the (set of) next type
value(s), then ~sample~ method returns (i+N)-th value instead of
(i+1)-th one.


~sample~ also replaces ~"done"~ with N-step version.


#+HTML: {{% notice warning %}} <p>
cpprb v10 no longer returns $\gamma ^{N-1}$ since users can always multiply fixed $\gamma ^N$.
#+HTML: </p>{{% /notice %}}


Since N-step buffer temporary store the values into local storage, you
need to call ~on_episode_end~ member function at the end of the every
episode end to flush into main storage properly.

| Parameters | Type                         | Description                |
|------------+------------------------------+----------------------------|
| ~size~     | ~int~                        | Nstep size                 |
| ~rew~      | ~str~ or array-like of ~str~ | Nstep reward               |
| ~gamma~    | ~float~                      | Discount factor            |
| ~next~     | ~str~ or array-like of ~str~ | Next items (e.g. next_obs) |


| Return Value               | Replace: From $\to$ To                        |
|----------------------------+-----------------------------------------------|
| Next items (e.g. next_obs) | $s_{t+1} \to s_{t+N}$                         |
| ~rew~                      | $r_t \to \sum _{n=0}^{N-1} \gamma ^n r_{t+n}$ |
| ~done~                     | $d_t \to 1-\prod _{n=0}^{N-1} (1-d_{t+n})$    |


*** Example Usage
#+begin_src python
import numpy as np
from cpprb import ReplayBuffer

nstep = 4
gamma = 0.99
discounts = gamma ** nstep

rb = ReplayBuffer(32,{'obs': {"shape": (4,4)},
                      'act': {"shape": 3},
                      'rew': {},
                      'next_obs': {"shape": (4,4)},
                      'done': {}},
                  Nstep={"size": nstep,
                         "gamma": gamma,
                         "rew": "rew",
                         "next": "next_obs"})

for i in range(100):
    done = 1.0 if i%10 == 9 else 0.0
    rb.add(obs=np.zeros((4,4)),
           act=np.ones((3)),
           rew=1.0,
           next_obs=np.zeros((4,4)),
           done=0.0)
    if done:
        rb.on_episode_end()

sample = rb.sample(16)

nstep_target = sample["rew"] + (1-sample["done"]) * discounts * Q(sample["next_obs"]).max(axis=1)
#+end_src

*** Notes
This N-step feature assumes sequential transitions in a trajectory
(episode) are stored sequentially. If you utilize distributed agent
configuration, you must add a single episode simultaneously.


*** Technical Detail

** DONE Large Batch Experience Replay
CLOSED: [2021-10-12 Tue 22:27]
:PROPERTIES:
:EXPORT_FILE_NAME: laber
:END:

*** Overview
Large Batch Experience Replay (LaBER) was proposed by T. Lahire /et al/.[fn:17]

The authors theoretically derived the best sampling probability \( p_i ^{\ast} \) to
minimize performance variance;

\[ p_i^{\ast} \propto \| \nabla _{\theta} L(Q_{\theta}(s_i, a_i), y_i) \| \text{,}\]

where \(L(\cdot,\, \cdot)\) is a loss function.

This requires full backpropagation and is costful, so that the authors
proposed surrogate priority
\(\hat{p}_i \propto \| \partial L(q_i, y_i) / \partial q_i \| \).

Since \( \| \nabla _{\theta} L(Q_{\theta}(s_i, a_i), y_i) \| = \| \partial L(q_i, y_i) / \partial q_i \cdot \nabla _{\theta} Q_{\theta}(s_i, a_i) \| \),
the surrogate priority is good approximation when
\( \nabla _{\theta} Q_{\theta}(s_i, a_i) \)
is almost constant across samples.

Moreover, when loss function is L2-norm, the surrogate priority
becomes TD error.

Although using TD error as priority is not so bad, one of the biggest
problems at PER is that the priorities are always outdated. However,
re-computing priorities of all transitions in the buffer at every
sampling is too expensive.

LaBER first samples \(m\)-times larger batch from the buffer
uniformly, then computes surrogate priorities for them, and samples
final batch according to the priorities.

According to the authors, LaBER can be used together with non-uniform
sampling like PER (they called it as PER-LaBER), however, the
combination doesn't improve the performance so much, even though there
are additional computational cost.


cpprb provides three helper classes =LaBERmean=, =LaBERlazy=, and
=LaBERmax=. If you don't have any special reasons, it is better to use
=LaBERmean=, which is theoretically and experimentally best. These
classes are constructed with following parameters;


| Parameters   | Type    | Description                                                 | Default |
|--------------+---------+-------------------------------------------------------------+---------|
| =batch_size= | =int=   | Desired final batch size (output size)                      |         |
| =m=          | =int=   | Multiplication factor (input size is =m * batch_size=)      | =4=     |
| =eps=        | =float= | Small positive constant to avoid 0 priority. (keyword only) | =1e-6=  |


After construction, these classes can be used as functor. You can call
with =priorities= keyword and any other optional environment values.

#+begin_src python
laber = LaBERmean(32, 4)

sample = laber(priorities= [ ... ], # 32 * 4 surrogate priorities.
               # optional: any additional environment values can be passed and subsampled together
               obs= # ...
               act= # ...
               ...
               )
#+end_src

*** Example Usage

The following pseudo code shows usage.

#+begin_src python
from cpprb import ReplayBuffer, LaBERmean

buffer_size = int(1e+6)
env_dict = # Define environment values

batch_size = 32
m = 4

n_iteration = int(1e+6)

rb = ReplayBuffer(buffer_size, env_dict)

laber = LaBERmean(batch_size, m)

env = # Create Env
policy = # Create Policy Network

observation = env.reset()
for _ in range(n_iteration):
    action = policy(observation)
    next_observation, reward, done, _ = env.step(action)

    rb.add(obs=observation,
           act=action,
           rew=reward,
           next_obs=next_observation,
           done=done)

    sample = rb.sample(batch_size * m)

    absTD = # Calculate surrogate priority using network

    idx_weights = laber(priorities=absTD)

    indexes = idx_weights["indexes"]
    weights = idx_weights["weights"]

    policy.train((absTD[indexes] * weights).mean())

    if done:
        observation = env.reset()
        rb.on_episode_end()
    else:
        observation = next_observation
#+end_src

The full example code are as follow;

#+INCLUDE: "../example/dqn-laber.py" src python

*** Notes
We add =eps= to avoid zero priority, however, the original
implementation don't have it. If you don't want to add small positive
constant, you can pass ~eps=0~ to the constructor (aka. =__init__=).

*** Technical Detail
Since the surrogate priority usually requires network's forward
caluculation, we implement LaBER separately from replay buffer.

Then =LaBERmean= etc. become simple classes, so that they are
implemented as ordinal Python classes insted of Cython =cdef= classes.


** DONE Reverse Experience Replay
CLOSED: [2021-12-18 Sat 16:40]
:PROPERTIES:
:EXPORT_FILE_NAME: rer
:END:


*** Overview
Reverse Experience Replay (RER) was proposed by E. Rotinov[fn:18] to
overcome environments with delayed (sparse) rewards. Since many
transitions doesn't have immediate reward, random sampling is
inefficient.

In RER, equally strided transitions are sampled from the latest
transition. The next sample contains one step older transions.

\[ \begin{align}
B_1 &= \lbrace& T_{t}  &, T_{t-stride}  &, \dots &, T_{t-batch~size \times stride}     &\rbrace \\
B_2 &= \lbrace& T_{t-1}&, T_{t-stride-1}&, \dots &, T_{t-batch~size \times stride - 1} &\rbrace \\
&\vdots&&&&&&
\end{align} \]

When the first sample index ($t-i$) becomes $2 \times stride$ old from
the latest transition, the first sample index is reset to the latest
transition.

#+CAPTION: Parameter Definition
| Parameters | Default | Description   |
|------------+---------+---------------|
| stride     |     300 | Sample stride |


*** Example Usage
The usage of =ReverseReplayBuffer= is same as the usage of ordinary =ReplayBuffer=.

#+begin_src python
from cpprb import ReverseReplayBuffer

buffer_size = 256
obs_shape = 3
act_dim = 1
stride = 20

rb = ReverseReplayBuffer(buffer_size,
                         env_dict = {"obs": {"shape": obs_shape},
                                     "act": {"shape": act_dim},
                                     "rew": {},
                                     "next_obs": {"shape": obs_shape},
                                     "done": {}},
                         stride = stride)

obs = np.ones(shape=(obs_shape))
act = np.ones(shape=(act_dim))
rew = 0
next_obs = np.ones(shape=(obs_shape))
done = 0

for i in range(500):
    rb.add(obs=obs,act=act,rew=rew,next_obs=next_obs,done=done)

    if done:
        # Together with resetting environment, call ReplayBuffer.on_episode_end()
        rb.on_episode_end()

batch_size = 32
sample = rb.sample(batch_size)
# sample is a dictionary whose keys are 'obs', 'act', 'rew', 'next_obs', and 'done'
#+end_src

*** Notes
The author indicated ~stride~ size must not be multiple of the length
of episode horizon to avoid sampling similar transitions
simultaneously.


*** Technical Detail


** DONE Hindsight Experience Replay (HER)
CLOSED: [2022-02-03 Thu 18:53]
:PROPERTIES:
:EXPORT_FILE_NAME: her
:END:

*** Overview
Hindsight Experience Replay (HER) was proposed by M. Andrychowicz /et al/.[fn:22]

The algorithm performs well for goal oriented, sparse binary reward
environment without manual reward shaping. (Moreover reward shaping
harms HER permormance.) HER utilizes unsuccessful episodes as if the
policy could reach their goals by re-labeling goal afterwards.

There are 4 strategies to pick up pseudo goals.

- final :: A goal become the end of the state in the episode.
- future :: ~k~-goals are randomly selected from future states in the episode.
- episode :: ~k~-goals are randomly selected from states in the episode.
- random :: ~k~-goals are randomly selected from all the states in the replay buffer.

Except for "final" strategy, hyper parameter ~k~ defines the number of
additional goals for each tansition. According to the authors,
"future" strategy with ~k=4~ or ~k=8~ performs best for "pusing",
"sliding", and "pick-and-place" environment.


cpprb provides =HindsightReplayBuffer= class. The class can be
constructed with following parameters;


| Parameter               | Type                                                         | Description                                                                                                    |
|-------------------------+--------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------|
| ~size~                  | ~int~                                                        | Buffer Size                                                                                                    |
| ~env_dict~              | ~dict~                                                       | Specifying environment. (See [[https://ymd_h.gitlab.io/cpprb/features/simple_usage/][here]]) ~"goal"~ and ~"rew"~ are added automatically, user should not specify them. |
| ~max_episode_len~       | ~int~                                                        | Maximum episode length                                                                                         |
| ~reward_func~           | ~Callable[[np.ndarray, np.ndarray, np.ndarray], np.ndarray]~ | Batch reward function of $S \times A \times G \to R$                                                           |
| ~goal_func=None~        | ~Callable[[np.ndarray], np.ndarray]~                         | Batch function for goal from state: $S \to G$. If ~None~ (default), state itself is considered as goal.        |
| ~goal_shape=None~       | ~Iterable[int]~                                              | Goal shape. If ~None~ (default), state shape is used.                                                          |
| ~state="obs"~           | ~str~                                                        | State name in ~env_dict~                                                                                       |
| ~action="act"~          | ~str~                                                        | Action name in ~env_dict~                                                                                      |
| ~next_state="next_obs"~ | ~str~                                                        | Next state name in ~env_dict~                                                                                  |
| ~strategy="future"~     | ~"future"~, ~"episode"~, ~"random"~, or ~"final"~            | Goal sampling strategy                                                                                         |
| ~additional_goals=4~    | ~int~                                                        | The number of additional goals per a transition                                                                |
| ~prioritized=True~      | ~bool~                                                       | Whether use PER                                                                                                |


The usage is similar to other Replay Buffer classes. One of the major
difference in usage is passing ~goal~ to ~on_episode_end~ method.

#+begin_src python
hrb.on_episode_end(goal)
#+end_src

*** Example Usage

The following pseudo code shows example usage;

#+begin_src python
from cpprb import HindsightReplayBuffer

buffer_size = 1e+6
nsteps = 1e+6
max_episode_len = 100
nwarmup = 100
batch_size = 32

hrb = HindsightReplayBuffer(buffer_size,
                            {"obs": {}, "act": {}, "next_obs": {}},
                            max_episode_len=max_episode_len,
                            reward_func=lambda s,a,g: -1*(s!=g))


env =  # Initialize Environment
policy =  # Initialize Policy

obs = env.reset()
goal = # Set Goal
for i in range(nsteps):
    act = policy(obs, goal)
    next_obs, _, done, _ = env.step(act)

    hrb.add(obs=obs, act=act, next_obs=next_obs)
    if done or (i >= max_episode_len):
        hrb.on_episode_end(goal)
        obs = env.reset()
        goal = # Set Goal
    else:
        obs = next_obs

    if hrb.get_stored_size() < nwarmup:
        continue

    sample = hrb.sample(batch_size)
    policy.train(sample)
#+end_src

The full example code is as follows;

#+INCLUDE: "../example/dqn-her.py" src python


*** Notes
The number of stored transitions (=get_stored_size()=) is increased by
calling =on_episode_end(goal)=.

*** Technical Detail
~HindsightReplayBuffer~ internally contains two replay buffers. The
one (=rb=) is for main buffer, the other (=episode_rb=) is for the
current episode. At first, transitions are inserted to
=episode_rb=. At the episode end (=on_episode_end=), goal relabeling
are executed, and those transitions are moved to =rb=.

This architecture might be helpful for similar needs such as
reward-to-go.

** DONE Memory Compression
CLOSED: [2019-11-10 Sun 13:33]
:PROPERTIES:
:EXPORT_FILE_NAME: memory_compression
:END:

Since replay buffer stores a large number of data set, memory
efficiency is one of the most important point.

In cpprb, there are two *optional* functionalities named ~next_of~ and
~stack_compress~, which you can turn on manually when constructing
replay buffer.

~next_of~ and ~stack_compress~ can be used together, but currently
none of them are compatible with N-step replay buffer.

These memory compressions rely on the internal memory alignment, so
that these functionalities cannot be used in situations where
sequential steps are not stored sequentially (e.g. distributed
reinforcement learning).

*** ~next_of~

**** Overview
In reinforcement learning, usually a set of observations before and
after a certain action are used for training, so that you save the set
in your replay buffer together. Naively speaking, all observations are
stored twice.

As you know, replay buffer is a ring buffer and the next value should
be stored at the next index, except for the newest edge.

If you specify ~next_of~ argument (whose type is ~str~ or array like
of ~str~), the "next value" of specified values are also created in
the replay buffer automatically and they share the memory location.

The name of the next value adds prefix ~next_~ to the original name
(e.g. ~next_obs~ for ~obs~, ~next_rew~ for ~rew~, and so on).

This functionality has small penalties for manipulating sampled index
and checking the cache for the newest index. (As far as I know, this
penalty is not significant, and you might not notice.)

**** Example Usage
#+begin_src python
import numpy as np
from cpprb import ReplayBuffer

buffer_size = 256

rb = ReplayBuffer(buffer_size,{"obs": {"shape": (84,84)},
                               "act": {"shape": 3},
                               "rew": {},
                               "done": {}}, # You must not specify "next_obs" nor "next_rew".
                  next_of=("obs","rew"))

rb.add(obs=np.ones((84,84)),
       act=np.ones(3),
       next_obs=np.ones((84,84)),
       rew=1,
       next_rew=1,
       done=0)
#+end_src

**** Notes
cpprb does not check the consistence of i-th ~next_foo~ and (i+1)-th
~foo~. This is user responsibility.

Since ~next_foo~ is automatically generated, you must not specify it
in the constructor manually.

**** Technical Detail
Internally, ~next_foo~ is not stored into a ring buffer, but into its chache.
(So still raising error if you don't pass them to ~add~.)

When sampling the ~next_foo~, indices (which is ~numpy.ndarray~) are
shifted (and wraparounded if necessary), then are checked whether they
are on the newest edge of the ring buffer. If the indices are on the
edge, the cached one is extracted.

*** ~stack_compress~

**** Overview
~stack_compress~ is designed for compressing stacked (or sliding
windowed) observation. A famous use case is Atari video game, where 4
frames of display windows are treated as a single observation and the
next observation is the one slided by only 1 frame
(e.g. 1,2,3,4-frames, 2,3,4,5-frames, 3,4,5,6-frames, ...). For this
example, a straight forward approach stores all the frames 4 times.

cpprb with ~stack_compress~ does not store duplicated frames in
stacked observation (except for the end edge of the internal ring
buffer) by utilizing numpy sliding trick.

You can specify ~stack_compress~ parameter, whose type is ~str~ or
array like of ~str~, at constructor.

**** Sample Usage
The following sample code stores ~4~-stacked frames of ~16x16~ data as
a single observation.

#+begin_src python
import numpy as np
from cpprb import ReplayBuffer

rb = ReplayBuffer(32,{"obs":{"shape": (16,16,4)}, 'rew': {}, 'done': {}},
                  next_of = "obs", stack_compress = "obs")

rb.add(obs=(np.ones((16,16,4))),
       next_obs=(np.ones((16,16,4))),
       rew=1,
       done=0)
#+end_src
**** Notes
In order to make compatible with [[https://github.com/openai/gym][OpenAI gym]], the last dimension is
considered as stack dimension (which is not fit to C array memory
order).

For the sake of performance, cpprb does not check the overlapped data
are truly identical, but simply overwrites with new data. Users must
not specify ~stack_compress~ for non-stacked data.

**** Technical Detail
Technically speaking ~numpy.ndarray~ (and other data type supporting
buffer protocol) has properties of item data type, the number of
dimensions, length of each dimension, memory step size of each
dimension, and so on. Usually, no data should overlap memory address,
however, ~stack_compress~ intentionally overlaps the memory addresses
in the stacked dimension.



** DONE Map Large Data on File
CLOSED: [2020-08-02 Sun 18:00]
:PROPERTIES:
:EXPORT_FILE_NAME: mmap
:END:

From version 9.2, ~ReplayBuffer~ accepts ~mmap_prefix~ keyword at
constructor. If ~mmap_prefix~ is specified, internal memory data are
mapped on files by using mmap.

This feature is useful when a required memory size is larger than
physical RAM size. Since the data is stored on file directly, this
might be slower.

The file names are ~f"{mmap_prefix}_{name}.dat"~.

** DONE Multiprocess Learning (Ape-X)
CLOSED: [2021-01-11 Mon 08:01]
:PROPERTIES:
:EXPORT_FILE_NAME: ape-x
:END:

From version 9.4.2, new classes ~MPReplayBuffer~ and
~MPPrioritizedReplayBuffer~ support multiprocess learning like [[https://arxiv.org/abs/1803.00933][Ape-X]]
(single learner with multiple explorers) on single machine
efficiently.

*** Shared Memory
First of all, ~MPReplayBuffer~ and ~MPPrioritizedReplayBuffer~ (Multi
Process ReplayBuffer) maps internal data on shared memory. This means
you don't need to use proxy
(e.g. ~multiprocessing.managers.SyncManager~) or queue
(e.g. ~multiprocessing.Queue~) for interproecss data sharing, but you
can simply access the buffer object from different process.

#+begin_src python
from multiprocessing import Process
from cpprb import MPPrioritizedReplayBuffer

rb = MPPrioritizedReplayBuffer(100,{"obs": {},"done" {}})

def explorer(rb):
    for _ in range(100):
        # Something ...
        rb.add(obs=obs, done=done)

p = Process(target=explorer,args=[rb]) # You can pass to Process simply as argument
p.start()
p.join()

sample = p.sample(10) # You can access data stored at different process.
#+end_src

*** Efficient Lock
Although you can implement Ape-X with ordinary =ReplayBuffer= or
=PrioritizedReplayBuffer= class, locking entire buffer when writing
and reading is quite inefficient.

#+begin_src python
# Part of Explorer Naiive Implementation
if local_buffer.get_stored_size() > local_size:
    local_sample = local_buffer.get_all_transitions()
    local_buffer.clear()

    with lock: # Inefficient: Lock entire buffer during addition
        global_buffer.add(**local_sample)
#+end_src

~MPReplayBuffer~ and ~MPPrioritizedReplayBuffer~ automatically lock
only critical section instead of entire buffer. For example, since
sequential ~add~ method calls should write different memory address,
its critical section is only index fetching and increment. This
locking reduction allows multiple explorers to add transitions
parallelly.[fn:1]

[[/cpprb/images/apex-per.png]]

We adopt exclusive-read concurrent-write model for access control. We
allow multiple writing parallelly and atomically trace the number of
writers in the critical section. Reading has higher priority and
prevents writers (aka. actors) from entering the critical section
again. When all writers exit the critical section, reader
(aka. learner) starts working in the critical section.

We restrict the number of learner to 1. If we allow multiple learners,
which have higher priorities, it is possible that actors will never
enter the critical section, which is not desired for reinforcement
learning.


*** Limitation
~MPReplayBuffer~ and ~MPPrioritizedReplayBuffer~ don't support
features of [[https://ymd_h.gitlab.io/cpprb/features/nstep/][Nstep Experience Replay]], [[https://ymd_h.gitlab.io/cpprb/features/memory_compression/][Memory Compression]], and [[https://ymd_h.gitlab.io/cpprb/features/mmap/][Map Data
on File]]. (You can still utilize these features at local buffers of
explorers.)

~MPReplayBuffer~ and ~MPPrioritizedReplayBuffer~ assume single
learner (~sample~ / ~update_priorities~) and multiple explorers
(~add~). You must not call learner functions from multiple processes
simultaneously.

*** Context and Backend
From version 10.6, ~MPReplayBuffer~ and ~MPPrioritizedReplayBuffer~
accept two new keyword arguments at their constructors.

| Name      | Default          | Type                                                        | Description                                                                                                                                                                                                                         |
|-----------+------------------+-------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ~ctx~     | ~None~           | ~Optional[Union[ForkContext,SpawnContext,SyncManager]]~     | Context to be used for ~Event~, ~Lock~ etc. If ~None~ (default), default context is used. The context passed must match with the context of ~Process~. When ~SyncManager~ is passed, ~Event~ and ~Lock~ are accessed through proxy. |
| ~backend~ | ~"sharedctypes"~ | ~"sharedctypes"~ or ~"SharedMemory"~ (only for Python 3.8+) | Backend for shared memory.                                                                                                                                                                                                          |



**** Context Detail
Context specifies how cpprb manages shared data and synchronization.

#+begin_src python
import multiprocessing as mp

default_context = mp.get_context()
fork_context = mp.get_context("fork") # only at Linux and macOS
spawn_context = mp.get_context("spawn")
#+end_src

On Linux, ~fork~ is the default context, where the parent process
itself is reused with copy-on-write strategy, so that usually
subprocess starts faster, but it might be problematic if some library
like TensorFlow has already started background thread.

On macOS and Linux, ~spawn~ is the default context, where only
necessary objects are copied to a fresh new process. The disadvantage
is not only slow start but also requiring users to define classes and
functions at top-level of module (See [[https://docs.python.org/3/library/pickle.html#what-can-be-pickled-and-unpickled][pickle]]).


Precisely speaking, ~SyncManager~ is not a start method like ~fork~
and ~spawn~. Actually, it starts new process with one of these
methods, then provides proxy objects to access the original objects
placed at the process. Generally, ~SyncManager~ is slower than others
because it requires interprocess communication. If you specifies
~SyncManager~, only synchronization objects (~Lock~ and ~Event~)
become proxy based, and internal main data are still placed on shared
memory.


**** Backend Detail
~SharedMemory~ was introduced at Python 3.8 ([[https://docs.python.org/3/library/multiprocessing.shared_memory.html][Ref]]). The backend can be
serialized, so that it can work with pickle based multiprocessing like
[[https://ray.io/][Ray]]. (See [[https://ymd_h.gitlab.io/cpprb/examples/mp_with_ray/][example]])

On Linux, ~SharedMemory~ are mapped under ~/dev/shm~ directory by
~shm_open~. If the directory doesn't have enough space, the program
dies with ~Bus error~, which (as far as we know) cannot be handled by
Python program correctly (like ~segmentation fault~). This is
often the case inside docker container. You can increase the size by
~docker run -it --shm-size 1G python3.9 bash~ etc.


On the other hand, ~sharedctypes~ cannot be serialized. It can be
passed to its subprocess only with process creation by ~fork~ and
~spawn~ (e.g. ~Process~).

The advantage of ~sharedctypes~ is that it has fallback mechanism
which can create shared memory even when ~/dev/shm~ directory doesn't
have enough space.


*** Example Code
#+INCLUDE: "../example/apex.py" src python

** DONE Save / Load Transitions
CLOSED: [2021-03-21 Sun 09:13]
:PROPERTIES:
:EXPORT_FILE_NAME: save_load_transitions
:END:

*** Overview

From version 10.1.0, ~ReplayBuffer~ (and its sub-classes) supports
save and load transitions (not entire buffer).

#+HTML: {{% notice warning %}} <p>
Since this functionality is based on Python pickle serialization, you
MUST NOT load untrusted file, otherwise you might introduce security
vulnerability.
#+HTML: </p>{{% /notice %}}


The API are ~save_transitions(self, file, *, safe=True)~ and
~load_transitions(self, file)~. The ~file~ parameter is
~str~. Internally, cpprb utilizes [[https://numpy.org/doc/stable/reference/generated/numpy.savez_compressed.html][numpy.savez_compressed]], so that the
file name have to end with ~".npz"~. (If not, automatically the suffix
is added.)

**** save
When ~safe=True~ (default), stored transitions are once dumped with
~get_all_transitions()~, then the well-organized transitions are
saved. This is much safer configuration in terms of future
compatibility. We highly recommend this whenever you can.

When ~safe=False~ and at least one of ~next_of~ and ~stack_compress~
are enabled, cpprb tries to reduce file size by dumping internal
compressed data structure directly. (If none of the options are
enabled, it fallbacks to ~safe=True~.) This also saves some internal
meta-data for reconstruction of transitions, so that it is possible
that file size can be larger than ~safe=True~ for small data.

**** load
You have to initialize ~ReplayBuffer~ with compatible configuration
before load transitions, otherwise you will get errors (~ValueError~,
~KeyError~, etc.) or unintentional silent bug.

~load_transitions~ does not overwrite existing transitions but adds,
so that if you want to delete, please call ~clear()~ manually.

*** Technical Detail

**** v1 format
| key     | value                                        | description                                                         |
|---------+----------------------------------------------+---------------------------------------------------------------------|
| safe    | ~True~ or ~False~                            |                                                                     |
| version | ~1~                                          |                                                                     |
| data    | ~dict[str, np.ndarray]~                      | If ~safe=True~, ~get_all_transitions()~. Otherwise internal buffer. |
| Nstep   | ~True~ or ~False~                            |                                                                     |
| cache   | ~dict[int, dict[str, np.ndarray]]~ or ~None~ | If ~safe=True~, ~None~. Otherwise internal cache.                   |
| next_of | ~np.ndarray~ or ~None~                       | If ~safe=True~, ~None~. Otherwise internal meta-data for ~next_of~. |



* Contributing
:PROPERTIES:
:EXPORT_HUGO_SECTION_FRAG: contributing
:END:

** DONE Step by Step Merge Request
CLOSED: [2020-01-17 Fri 23:09]
:PROPERTIES:
:EXPORT_FILE_NAME: merge_request
:END:

The first step of coding contribution is to fork cpprb on GitLab.com.

The detail steps for fork is described at [[https://docs.gitlab.com/ee/gitlab-basics/fork-project.html][official document]].

After fork cpprb on the web, you can clone repository to your local
machine and set original cpprb as "upstream" by

#+begin_src shell
git clone https://gitlab.com/<Your GitLab Account>/cpprb.git
cd cpprb
git remote add upstream https://gitlab.com/ymd_h/cpprb.git
#+end_src

To make "master" branch clean, you need to create new branch before you edit.

#+begin_src shell
git checkout -b <New Branch Name> master
#+end_src

This process is necessay because "master" and other original branches
might progress during your working.


From here, you can edit codes and make commit as usual.


After finish your work, you must recheck original cpprb and ensure
there is no cnflict.

#+begin_src shell
git pull upstream master
git checkout <Your Branch Name>
git merge master # Fix confliction here!
#+end_src


If everything is fine, you push to your cpprb.

#+begin_src shell
git push origin <Your Branch Name>
#+end_src

Merge request can be created from the web, the detail is described at
[[https://docs.gitlab.com/ee/user/project/merge_requests/creating_merge_requests.html][official document]].


There is [[https://stackoverflow.com/a/14681796][a good explanation]] for making good Pull Request (merge
request equivalent on GitHub.com)

** DONE Developer Guide
CLOSED: [2021-10-08 Fri 12:18]
:PROPERTIES:
:EXPORT_FILE_NAME: dev_guide
:END:

*** Source Code Layout
The main source code of cpprb is =cpprb/PyReplayBuffer.pyx=. Almost
all the public classes and functions are implemented in this
file. This file contains more than 2K lines (including blank lines),
so that maybe we should divide it. However, if we want to utilize full
=cdef= functionality, (as far as we know) we have to keep =def= in
=.pyx= and =cdef= in =.pxd= separately. We feel that makes much harder
to maintain and just put everything in the single file.

=cpprb/VectorWrapper.pyx= and =cpprb/VectorWrapper.pxd= have interface
classes between =.pyx= and C++. These classes utilize [[https://docs.python.org/ja/3/c-api/buffer.html][buffer protocol]],
which enables for us to share large data without copy.


=cpprb/ReplayBuffer.hh= contains C++ implementation of PER
sampling. In this file, there are a lot of template and SFINAE
meta-programming. By using =if constexpr=, multi-thread
(multi-process) version switches algorithm without runtime cost. In
order to support multi-process shared memory, the classes can accept
(shared) memory acquired at Python side.


=cpprb/SegmentTree.hh= is the one of the oldest code and has segment
tree implementation. It can accept shared memory for multi-processing,
too. The multi-processing version updates its tree lazily. Internal
data structure is described at [[https://ymd_h.gitlab.io/cpprb/features/ape-x/][here]].


=cpprb/ReplayBuffer.pxd= defines interface classes and functions of
=cpprb/ReplayBuffer.hh= in order to use them in
=cpprb/PyReplayBuffer.pyx=.


=test/*.py= have unit tests. A lot of regression tests are included,
too. =Dockerfile= is used for build and test
environment. =.coveragerc= defines coverage configuration for
[[https://coverage.readthedocs.io][Coverage.py]].


Under =site= directory, there are project site source and config file.
=sphinx= directory contains main page and config file of API
documentation. These documentation system is described "Documentation"
section below.

=example= directory has example codes. We need more examples.


=CHANGELOG.org= contains change log.


=CITATION.cff= is used for citation management at GitHub. Please read
[[https://github.blog/2021-08-19-enhanced-support-citations-github/][GitHub official blog post]].


=setup.py= is used for package build. =MANIFEST.in= specifies package
additional files.


=.gitlab-ci.yml= defines CI on GitLab CI/CD platform. =.github=
directory contains CI config files for GitHub Actions. CI settings are
described at "CI Configuration" section.


=crean_reinstall.sh= is old build script for local
development. Recently, we don't use it and we might remove it in the
future.

*** Build System
Build system is defined at =setup.py=. We don't use =pyproject.toml=
nor =requirements.txt=. We know declarative programming is preferable,
however, it cannot handle complicated conditional build rules,
yet. For example, we would like to use Cython only when =.pyx= files
exist and they are newer than corresponding =.cpp= ones. Source codes
hosted at PyPI (not GitLab/GitHub) don't include =.pyx=, then users
who use pip command don't need to prepare Cython at all.

In order to realize single command installation, NumPy and Cython are
lazily imported after =setup_requires= become available.

Generated markdown of README is re-used as =long_description=
describing PyPI package page. A unrosolved problem is that we cannot
show the cpprb logo on the PyPI page.

Additionally, by setting =DEBUG_CPPRB= environment value, we enable
debug mode build, where Cython line trace is enabled.

*** Documentation
Project site are built by [[https://gohugo.io/][Hugo]] and hosted at [[https://docs.gitlab.com/ee/user/project/pages/][GitLab Pages]]. The
documents are written in [[https://www.orgmode.org/index.html][Org Mode]] (=site/site.org=) and exported with
[[https://ox-hugo.scripter.co/][ox-hugo]]. (aka. =.org= -> =.md= -> =.html=) This sequence is very
useful to reuse (partial) README in the project site. The disadvantage
is Org Mode is minor to markdown. =site/init.el= defines ox-hugo
configuration.

We use [[https://learn.netlify.app/en/][Hugo Learn Theme]]. We also make custom shortcodes at
=site/layouts/shortcodes= in order to create the highly customized
[[https://ymd_h.gitlab.io/cpprb/][landing page]]. We still need design improvement to make it easier and
more understandable for new commers.

To show star and fork counts on the side bar, we use custom JavaScript
defined at =site/static/js/custom.js=. We utilize [[https://docs.gitlab.com/ee/api/api_resources.html][GitLab REST API]] and
[[https://docs.github.com/en/rest][GitHub REST API]].

[[https://ymd_h.gitlab.io/cpprb/api/index.html][API reference]] is built with [[https://www.sphinx-doc.org/en/master/index.html][Sphinx]] from docstring at the source
codes. The docstring adopts [[https://numpydoc.readthedocs.io/en/latest/format.html][NumPy style]], so that [[https://sphinxcontrib-napoleon.readthedocs.io/en/latest/][Napoleon extension]] is
enabled. By using custom CSS (aka. =sphinx/static/custom.css=), we try
to match the look and feel with that of the Hugo part. Additionally,
we use [[https://sphinx-automodapi.readthedocs.io/en/latest/][sphinx-automodapi]] to build the module structure.

[[https://ymd_h.gitlab.io/cpprb/coverage/][Unit test coverage report]] are hosted, too. The report is generated by
unit test.


*** CI Configuration
We mainly utilize [[https://docs.gitlab.com/ee/ci/][GitLab CI/CD]]. We execute binary build, unit test,
test coverage aggregation, PyPI upload, and project site build and
deploy. The configuration is defined at =.gitlab-ci.yml=.

Unfortunately, schedule trigger jobs have not been working
recently. We have to investigate it.

Additionally, [[https://github.co.jp/features/actions][GitHub Actions]] are used for specific purpose. Since we
can use Windows and macOS for free, we execute test script on them and
upload Windows binary to PyPI.

We utilize [[https://codeql.github.com/][CodeQL]] for code security and quality analysis. The results
are shown [[https://github.com/ymd-h/cpprb/security/code-scanning][here]] (Is the page public?). To be honest, we are not sure
whether this analysis is meaningful for us.


* DONE Examples
CLOSED: [2020-02-15 Sat 09:23]
:PROPERTIES:
:EXPORT_HUGO_SECTION_FRAG: examples
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_WEIGHT: 800
:END:

- [[https://ymd_h.gitlab.io/cpprb/examples/dqn_per/][DQN with PrioritizedReplayBuffer]]
- [[https://ymd_h.gitlab.io/cpprb/examples/lap/][Loss Adjusted Prioritization (LAP)]]
- [[https://ymd_h.gitlab.io/cpprb/examples/ape-x/][Multiprocess Learning (Ape-X) with MPPrioritizedReplayBuffer]]
- [[https://ymd_h.gitlab.io/cpprb/examples/mp_with_ray/][Use MPReplayBuffer with Ray]]
- [[https://ymd_h.gitlab.io/cpprb/examples/laber/][Large Batch Experience Replay (LaBER)]]
- [[https://ymd_h.gitlab.io/cpprb/examples/her/][Hindsight Experience Replay (HER)]]
- [[https://ymd_h.gitlab.io/cpprb/examples/env_helper/][Create ReplayBuffer for non-simple gym.Env with helper functions]]


* DONE DQN with ~PrioritizedReplayBuffer~
CLOSED: [2022-02-25 Fri 07:45]
:PROPERTIES:
:EXPORT_HUGO_SECTION_FRAG: examples
:EXPORT_FILE_NAME: dqn_per
:END:

The following is a DQN[fn:11] example. This also includes Double DQN[fn:10] and
Prioritized Experience Replay[fn:3].

#+INCLUDE: "../example/dqn.py" src python

* DONE Loss Adjusted Prioritization
CLOSED: [2022-02-25 Fri 07:48]
:PROPERTIES:
:EXPORT_HUGO_SECTION_FRAG: examples
:EXPORT_FILE_NAME: lap
:END:

The following is a LAP[fn:9] example.
See [[https://ymd_h.gitlab.io/cpprb/understanding/#relation-between-loss-functions-and-non-uniform-sampling][the theory page]].

#+INCLUDE: "../example/dqn-lap.py" src python


* DONE Multiprocess Learning (Ape-X) with ~MPPrioritizedReplayBuffer~
CLOSED: [2022-02-25 Fri 07:48]
:PROPERTIES:
:EXPORT_HUGO_SECTION_FRAG: examples
:EXPORT_FILE_NAME: ape-x
:END:
This is a Ape-X[fn:12] example code.
See [[https://ymd_h.gitlab.io/cpprb/features/ape-x/][Multiprocess Learning (Ape-X)]] for detail.

#+INCLUDE: "../example/apex.py" src python

* DONE Use ~MPReplayBuffer~ with Ray
CLOSED: [2022-02-25 Fri 12:38]
:PROPERTIES:
:EXPORT_HUGO_SECTION_FRAG: examples
:EXPORT_FILE_NAME: mp_with_ray
:END:

#+HTML: {{% notice note %}} <p>
Please see [[https://ymd_h.gitlab.io/cpprb/features/ape-x/#context-and-backend][this document]], too.
#+HTML: </p>{{% /notice %}}


[[https://ray.io/][Ray]] is a OSS framework which enables users to build distributed
application easily.

Ray can utilize multiple machines, so that Ray architecture doesn't
use shared memory except for immutable objects ([[https://ray-project.github.io/2017/08/08/plasma-in-memory-object-store.html][Ref]]). However, if you
use only a single machine, you might want to use shared memory for
replay buffer to avoid expensive interprocess data sharing.

With cpprb 10.6+ and Python 3.8+, you can use ~MPReplayBuffer~ and
~MPPrioritizedReplayBuffer~ with Ray.

A key trick is to set ~authkey~ inside [[https://docs.ray.io/en/latest/ray-core/actors.html][Ray Actors]], which allows the Ray
worker to communicate with ~SyncManager~ process.

#+begin_src python
import base64
import multiprocessing as mp

import ray

ray.init()

@ray.remote
class RemoteWorker:
    # Encode base64 to avoid following error:
    #   TypeError: Pickling an AuthenticationString object is disallowed for security reasons
    encoded = base64.b64encode(mp.current_process().authkey)

    def __init__(self):
        mp.current_process().authkey = base64.b64decode(self.encoded)

    def run(self, some_resource):
        pass

w = RemoteWorker.remote()
#+end_src

#+HTML: {{% notice warning %}} <p>
This trick overwrites process-wide ~authkey~, which might confilict if
you use other processes in it.
#+HTML: </p>{{% /notice %}}



We also have to select ~SharedMemory~ backend and ~SyncManager~
context. By this configuration, main data is placed on shared memory
and synchronization objects (e.g. ~Lock~ and ~Event~) are accessed
through ~SyncManager~ proxy.

#+begin_src python
import multiprocessing as mp

from cpprb import MPReplayBuffer

buffer_size = 1e+6
m = mp.get_context().Manager() # SyncManager

rb = MPReplayBuffer(buffer_size,
                    {"done": {}},
                    ctx = m, # Context
                    backend="SharedMemory")
#+end_src


In the end, (pseudo) example become like this;

#+INCLUDE: "../example/multiprocessing-ray.py" src python


* DONE Large Batch Experience Replay (LaBER)
CLOSED: [2022-02-25 Fri 07:49]
:PROPERTIES:
:EXPORT_HUGO_SECTION_FRAG: examples
:EXPORT_FILE_NAME: laber
:END:
The following is a LaBER[fn:17] example code.
See [[https://ymd_h.gitlab.io/cpprb/features/laber/][Large Batch Experience Replay]] for detail.

#+INCLUDE: "../example/dqn-laber.py" src python

* DONE Hindsight Experience Replay (HER)
CLOSED: [2022-02-25 Fri 07:49]
:PROPERTIES:
:EXPORT_HUGO_SECTION_FRAG: examples
:EXPORT_FILE_NAME: her
:END:
The following is a HER[fn:22] example code.
See [[https://ymd_h.gitlab.io/cpprb/features/her/][Hindsight Experience Replay]] for detail.

#+INCLUDE: "../example/dqn-her.py" src python


* DONE Create ~ReplayBuffer~ for non-simple =gym.Env= with helper functions
CLOSED: [2022-02-25 Fri 07:50]
:PROPERTIES:
:EXPORT_HUGO_SECTION_FRAG: examples
:EXPORT_FILE_NAME: env_helper
:END:

#+INCLUDE: "../example/create_buffer_with_helper_func.py" src python

* Comparison
:PROPERTIES:
:EXPORT_HUGO_SECTION_FRAG: comparison
:EXPORT_HUGO_WEIGHT: 850
:END:

** DONE Comparison
CLOSED: [2020-02-16 Sun 23:08]
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:END:

In this section, we compare cpprb with other replay buffer implementations;

- [[https://github.com/openai/baselines][OpenAI Baselines]]
  - [[https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py][baselines.deepq.replay_buffer.ReplayBuffer]]
  - [[https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py][baselines.deepq.replay_buffer.PrioritizedReplayBuffer]]
- [[https://github.com/ray-project/ray][Ray RLlib]]
  - [[https://github.com/ray-project/ray/blob/master/rllib/execution/replay_buffer.py][ray.rilib.execution.replay_buffer.ReplayBuffer]]
  - [[https://github.com/ray-project/ray/blob/master/rllib/execution/replay_buffer.py][ray.rllib.execution.replay_buffer.PrioritizedReplayBuffer]]
- [[https://github.com/chainer/chainerrl][Chainer ChainerRL]]
  - [[https://github.com/chainer/chainerrl/blob/master/chainerrl/replay_buffers/replay_buffer.py][chainerrl.replay_buffers.ReplayBuffer]]
  - [[https://github.com/chainer/chainerrl/blob/master/chainerrl/replay_buffers/prioritized.py][chainerrl.replay_buffers.PrioritizedReplayBuffer]]
- [[https://github.com/deepmind/reverb][DeepMind Reverb]]


*Important Notice*

Except cpprb and DeepMind/Reverb, replay buffers are only a part of
their reinforcement learning ecosystem. These libraries don't focus on
providing greatest replay buffers but reinforcement learning.

Our motivation is to provide strong replay buffers to researchers and
developers who not only use existing networks and/or algorithms but
also creating brand-new networks and/or algorithms.

Here, we would like to show that cpprb is enough functional and enough
efficient compared with others.

*** OpenAI Baselines
[[https://github.com/openai/baselines][OpenAI Baselines]] is a set of baseline implementations of reinforcement
learning developed by OpenAI.

The source code is published with MIT license.

Ordinary and prioritized experience replay are implemented with
~ReplayBuffer~ and ~PrioritizedReplayBuffer~ classes, respectively.
Using these classes directly is (probably) not expected, but you can
import them like this;

#+begin_src python
from baselines.deepq.replay_buffer import ReplayBuffer, PrioritizedReplayBuffer
#+end_src

~ReplayBuffer~ is initialized with ~size~ parameter for replay buffer
size. Additionally, ~PrioritizedReplayBuffer~ requires ~alpha~
parameter for degree of prioritization, too. These parameters doesn't
have default values, so that you need to specify them.

#+begin_src python
buffer_size = int(1e6)
alpha = 0.6

rb = ReplayBuffer(buffer_size)
prb = PrioritizedReplayBuffer(buffer_size,alpha)
#+end_src

A transition is stored into replay buffer by calling
~ReplayBuffer.add(self,obs_t,action,reward,obs_tp1,done)~.

For ~PrioritizedReplayBuffer~, the maximum priority at that time is
automatically used for a newly added transition.

These replay buffers are ring buffers, so that the oldest transition
is overwritten by a new transition after the buffer becomes full.

#+begin_src python
obs_t = [0, 0, 0]
action = [1]
reward = 0.5
obs_tp1 = [1, 1, 1]
done = 0.0

rb.add(obs_t,action,reward,obs_tp1,done)
prb.add(obs_t,action,reward,obs_tp1,done) # Store with max. priority
#+end_src

Stored transitions can be sampled by calling
~ReplayBuffer.sample(self,batch_size)~ or
~PrioritizedReplayBuffer.sample(self,batch_size,beta)~.

~ReplayBuffer~ returns a tuple of batch size
transition. ~PrioritizedReplayBuffer~ also returns weights and
indexes, too.

#+begin_src python
batch_size = 32
beta = 0.4

obs_batch, act_batch, rew_batch, next_obs_batch, done_mask = rb.sample(batch_size)
obs_batch, act_batch, rew_batch, next_obs_batch, done_mask, weights, idxes = prb.sample(batch_size)
#+end_src

Priorities can be updated by calling
~PrioritizedReplayBuffer.update_priorities(self,idxes,priorities)~.

#+begin_src python
prb.update_priorities(idxes,priorities)
#+end_src

Internally, these replay buffers utilize Python ~list~ for storage, so
that the memory usage gradually increase until the buffer becomes
full.

*** Ray RLlib
[[https://docs.ray.io/en/latest/rllib.html][RLlib]] is reinforcement learning library based on distributed framework
[[https://github.com/ray-project/ray][Ray]].

The source code is published with Apache-2.0 license.

Ordinary and prioritized experience replay are implemented with
~ReplayBuffer~ and ~PrioritizedReplayBuffer~ classes, respectively.

These classes are decorated with ~@DeveloperAPI~, which are intended
to be used by developer when making custom algorithms.

#+begin_src python
from ray.rllib.execution.replay_buffer import ReplayBuffer, PrioritizedReplayBuffer
#+end_src

These replay buffer classes initialize like OpenAI Baselines;

#+begin_src python
buffer_size = int(1e6)
alpha = 0.6

rb = ReplayBuffer(buffer_size)
prb = PrioritizedReplayBuffer(buffer_size,alpha)
#+end_src

A transition is stored by calling ~ReplayBuffer.add(self,item,weight)~
and ~PrioritizedReplayBuffer.add(self,item,weight)~. The ~item~ is a
instance of ~ray.rllib.policy.sample_batch.SampleBatch~. (The API was
changed at [[https://github.com/ray-project/ray/releases/tag/ray-0.8.7][ray-0.8.7]].)

The ~SampleBatch~ class can have any kind of values with batch size,
however, only a single transition is allowed for addition. (If you
pass ~SampleBatch~ having multi-transitions to ~add()~ method, it will
produce unintentional bug probably.) The key of ~SampleBatch~ can be
defined by user freely.

In RLlib, ~PrioritizedReplayBuffer~ can take ~weight~ parameter to
specify priority at the same time. In order to unify their API,
~ReplayBuffer~ also requires ~weight~ parameter, even though it is not
used at all. Moreover, the ~weight~ does not have default parameter
(such as ~None~), you need to pass something explicitly.

#+begin_src python
obs_t = [0, 0, 0]
action = [1]
reward = 0.5
obs_tp1 = [1, 1, 1]
done = 0.0
weight = 0.5

# For addition, SampleBatch must be initialized with a set of `list` having only a single element.
rb.add(SampleBatch(obs=[obs_t],
                   action=[action],
                   reward=[reward],
                   new_obs=[obs_tp1],
                   done=[done]),None)
prb.add(SampleBatch(obs=[obs_t],
                    action=[action],
                    reward=[reward],
                    new_obs=[obs_tp1],
                    done=[done]),weight)
#+end_src


Like OpenAI Baselines, stored transitions can be sampled by calling
~ReplayBuffer.sample(self,batch_size)~ or
~PrioritizedReplayBuffer.sample(self,batch_size,beta)~.

~ReplayBuffer~ returns ~SampleBatch~ of batch size transition. (The
API was changed at [[https://github.com/ray-project/ray/releases/tag/ray-0.8.7][ray-0.8.7]].) ~PrioritizedReplayBuffer~ also returns
~weights~ and ~batch_indexes~ in ~SampleBatch~, too.

#+begin_src python
batch_size = 32
beta = 0.4

sample = rb.sample(batch_size)
obs_batch, act_batch, rew_batch, next_obs_batch, done_mask = sample["obs"], sample["action"], sample["reward"], sample["new_obs"], sample["done"]

sample = prb.sample(batch_size)
obs_batch, act_batch, rew_batch, next_obs_batch, done_mask, weights, idxes = sample["obs"], sample["action"], sample["reward"], sample["new_obs"], sample["done"], sample["weights"], sample["batch_indexes"]
#+end_src

Priorities can be also updated by calling
~PrioritizedReplayBuffer.update_priorities(self,idxes,priorities)~,
too.

#+begin_src python
prb.update_priorities(idxes,priorities)
#+end_src

Internally, these replay buffers utilize Python ~list~ for storage, so
that the memory usage gradually increase until the buffer becomes
full.

*** Chainer ChainerRL
[[https://github.com/chainer/chainerrl][ChainerRL]] is a deep reinforcement learning library based on a
framework [[https://github.com/chainer/chainer][Chainer]]. Chainer (including ChainerRL) has already stopped
active development, and development team (Preferred Networks) joined
to [[https://pytorch.org/][PyTorch]] development.

The source code is published with MIT license.

Ordinary and prioritized experience replay are implemented with
~ReplayBuffer~ and ~PrioritizedReplayBuffer~, respectively.

#+begin_src python
from chainerrl.replay_buffers import ReplayBuffer, PrioritizedReplayBuffer
#+end_src

ChainerRL has slightly different API from OpenAI Baselines' one.

~ReplayBuffer~ is initialized with ~capacity=None~ parameter for
buffer size and ~num_steps=1~ parameter for Nstep configuration.

~PrioritizedReplayBuffer~ can additionally take parameters of
~alpha=0.6~, ~beta0=0.4~, ~betasteps=2e5~, ~eps=0.01~,
~normalize_by_max=True~, ~error_min=0~, ~error_max=1~.

In ChainerRL, beta (correction for weight) parameter starts from
~beta0~, automatically increases with equal step size during
~betasteps~ time iterations and after that becomes ~1.0~.

#+begin_src python
buffer_size = int(1e6)
alpha = 0.6

rb = ReplayBuffer(buffer_size)
prb = PrioritizedReplayBuffer(buffer_size,alpha)
#+end_src

A transition is stored by calling
~ReplayBuffer.append(self,state,action,reward,next_state=None,next_action=None,is_state_terminate=False,env_id=0,**kwargs)~.
Additional keyward arguments are stored, too, so that you can use any
custom environment values. By specifying ~env_id~, multiple trajectory
can be tracked with Nstep configuration.

#+begin_src python
obs_t = [0, 0, 0]
action = [1]
reward = 0.5
obs_tp1 = [1, 1, 1]
done = False

rb.add(obs_t,action,reward,obs_tp1,is_state_terminal=done)
prb.add(obs_t,action,reward,obs_tp1,is_state_terminal=done)
#+end_src

Stored transitions are sampled by calling
~ReplayBuffer.sample(self,num_experience)~ and
~PrioritizedReplayBuffer.sample(self,n)~.

Apart from other implementations, ChainerRL's replay buffers return
unique (non-duplicated) transitions, so that batch_size must be
smaller than stored transition size. Furthermore, they return a Python
~list~ of a ~dict~ of transition instead of a Python ~tuple~ of
environment values.

#+begin_src python
batch_size = 32

# Need additional modification to take apart
transitions = rb.sample(batch_size)
transitions_with_weight = prb.sample(batch_size)
#+end_src

Update index cannot be specified manually, but
~PrioritizedReplayBuffer~ memorizes sampled indexes.
(Without ~sample~, user cannot update priority.)

#+begin_src python
prb.update_priorities(priorities)
#+end_src


Internally, these replay buffers utilize Python ~list~ for storage, so
that the memory usage gradually increase until the buffer becomes
full. In ChainerRL, storage is not a simple Python ~list~, but two
~list~ to pop out the oldest element with O(1) time.

*** DeepMind Reverb
[[https://github.com/deepmind/reverb][Reverb]] is relatively new, which was released on 26th May 2020 by
DeepMind.

Reverb is a framework for experience replay like cpprb. By utilizing
server-client model, Reverb is mainly optimized for large-scale
distributed reinforcement learning.

The source code is published with Apache-2.0 license.

Currently (28th June 2020), Reverb officially says it is still
non-production level and requires a development version of TensorFlow
(i.e. tf-nightly 2.3.0.dev20200604).

Ordinary and prioritized experience replay are constructed by setting
~reverb.selectors.Uniform()~ and ~reverb.selectors.Prioritized(alpha)~
to ~sampler~ argument in ~reverb.Table~ constructor, respectively.

Following sample code constructs a server with two replay buffers
listening a port ~8000~.

#+begin_src python
import reverb

buffer_size = int(1e6)
alpha = 0.6

server = reverb.Server(tables=[reverb.Table(name="ReplayBuffer",
                                            sampler=reverb.selectors.Uniform(),
                                            remover=reverb.selectors.Fifo(),
                                            rate_limiter=reverb.rate_limiters.MinSize(1),
                                            max_size=buffer_size),
                               reverb.Table(name="PrioritizedReplayBuffer",
                                            sampler=reverb.selectors.Prioritized(alpha),
                                            remover=reverb.selectors.Fifo(),
                                            rate_limiter=reverb.rate_limiters.MinSize(1),
                                            max_size=buffer_size)],
                       port=8000)
#+end_src

By changing ~selector~ and ~remover~, we can use different algorithms
for sampling and overwriting, respectively.

Supported algorithms implemented at ~reverb.selectors~ are following;

- ~Uniform~: Select uniformly.
- ~Prioritized~: Select proportional to stored priorities.
- ~Fifo~: Select oldest data.
- ~Lifo~: Select newest data.
- ~MinHeap~: Select data with lowest priority.
- ~MaxHeap~: Select data with highest priority.


There are 3 ways to store a transition.

The first method uses ~reverb.Client.insert~. Not only prioritized
replay buffer but also ordinary replay buffer requires priority even
though it is not used.

#+begin_src python
import reverb

client = reverb.Client(f"localhost:{server.port}")

obs_t = [0, 0, 0]
action = [1]
reward = [0.5]
obs_tp1 = [1, 1, 1]
done = [0]

client.insert([obs_t,action,reward,obs_tp1,done],priorities={"ReplayBuffer":1.0})
client.insert([obs_t,action,reward,obs_tp1,done],priorities={"PrioritizedReplayBuffer":1.0})
#+end_src

The second method uses ~reverb.Client.writer~, which is internally
used in ~reverb.Client.insert~, too. This method can be more efficient
because you can flush multiple items together by calling
~reverb.Writer.close~ insead of one by one.

#+begin_src python
import reverb

client = reverb.Client(f"localhost:{server.port}")

obs_t = [0, 0, 0]
action = [1]
reward = [0.5]
obs_tp1 = [1, 1, 1]
done = [0]

with client.writer(max_sequence_length=1) as writer:
    writer.append([obs_t,action,reward,obs_tp1,done])
    writer.create_item(table="ReplayBuffer",num_timesteps=1,priority=1.0)

    writer.append([obs_t,action,reward,obs_tp1,done])
    writer.create_item(table="PrioritizedReplayBuffer",num_timesteps=1,priority=1.0)
#+end_src

The last method uses ~reverb.TFClient.insert~. This class is designed
to be used in TensorFlow graph.

#+begin_src python
import reverb

tf_client = reverb.TFClient(f"localhost:{server.port}")

obs_t = tf.constant([0, 0, 0])
action = tf.constant([1])
reward = tf.constant([0.5])
obs_tp1 = tf.constant([1, 1, 1])
done = tf.constant([0])

tf_client.insert([obs_t,action,reward,obs_tp1,done],
                 tables=tf.constant(["ReplayBuffer"]),
                 priorities=tf.constant([1.0],dtype=tf.float64))
tf_client.insert([obs_t,action,reward,obs_tp1,done],
                 tables=tf.constant(["PrioritizedReplayBuffer"]),
                 priorities=tf.constant([1.0],dtype=tf.float64))
#+end_src

~tables~ parameter must be ~tf.Tensor~ of ~str~ with rank 1 and
~priorities~ parameter must be ~tf.Tensor~ of ~float64~ with
rank 1. The lengths of ~tables~ and ~priorities~ must match.


Sampling transitions can be realized by 3 ways, too.

The first method utilizes ~reverb.Client.sample~, which returns
generator of ~reverb.replay_sample.ReplaySample~. As long as we can
investigate, beta-parameter is not supported and weight is not
calculated at prioritized experience replay.

#+begin_src python
batch_size = 32

transitions = client.sample("ReplayBuffer",num_samples=batch_size)
transitions_with_priority = client.sample("PrioritizedReplayBuffer",num_samples=batch_size)
#+end_src


The second method uses ~reverb.TFClient.sample~, which does not
support batch sampling.

#+begin_src python
transition = tf_client.sample("ReplayBuffer",
                              [tf.float64,tf.float64,tf.float64,tf.float64,tf.float64])
transition_priority = tf_client.sample("PrioritizedReplayBuffer",
                                       [tf.float64,tf.float64,tf.float64,tf.float64,tf.float64])
#+end_src


The last method is completely different from others, which calls
~reverb.TFClient.dataset~, returning ~reverb.ReplayDataset~ derived
from ~tf.data.Dataset~.

Once creating ~ReplayDataset~, the dataset can be used as generator
and automatically fetches transitions from its replay buffer with
proper timing.


#+begin_src python
dataset = tf_client.dataset("ReplayBuffer",
                            [tf.float64,tf.float64,tf.float64,tf.float64,tf.float64],
                            [4,1,1,4,1])
dataset_priority = tf_client.dataset("PrioritizedReplayBuffer",
                                     [tf.float64,tf.float64,tf.float64,tf.float64,tf.float64],
                                     [4,1,1,4,1])
#+end_src


Priorities can be updated by ~reverb.Client.mutate_priorities~ or
~reverb.TFClient.update_priorities~. Aside from other implementations,
key is not integer sequence but hash, so that the key must be taken
from sampled items by accsessing ~ReplaySample.info.key~.

#+begin_src python
for t in transitions_with_priority:
    client.mutate_priorities("PriorotizedReplayBuffer",updates={t.info.key: 0.5})

tf_client.update_priorities("PrioritizedReplayBuffer",
                            transition_priority.info.key,
                            priorities=tf.constant([0.5],dtype=tf.float64)
#+end_src

*** Other Implementations
There are also some other replay buffer implementations, which we
couldn't review well. In future, we would like to investigate these
implementations and compare with cpprb.

- [[https://github.com/tensorflow/agents][TF-Agents]] :: TensorFlow official reinforcement learning library
- [[https://github.com/google-research/seed_rl][SEED RL]] :: Scalable and Effificient Deep-RL
- [[https://github.com/tensorflow/models][TensorFlow Model Garden]] :: TensorFlow example implementation


** DONE Functionality
CLOSED: [2020-02-24 Mon 12:49]
:PROPERTIES:
:EXPORT_FILE_NAME: functionality
:EXPORT_HUGO_WEIGHT: 10
:END:

The following table summarizes functionalities of replay buffers.

|                           | cpprb                                 | OpenAI/Baselines                          | Ray/RLlib                                                  | Chainer/ChainerRL          | DeepMind/Reverb |
|---------------------------+---------------------------------------+-------------------------------------------+------------------------------------------------------------+----------------------------+-----------------|
| *Flexible Environment*    | Yes                                   | No                                        | No                                                         | Yes                        | Yes             |
| *Nstep*                   | Yes                                   | No                                        | Yes                                                        | Yes                        | No              |
| *Parellel Exploration*    | Yes (Support Ape-X on single machine) | Yes (Avarages gradients of MPI processes) | Yes (Concatenates sample batches from distributed buffers) | No                         | Yes             |
| *Save/Load*               | Only transitions ([[https://ymd_h.gitlab.io/cpprb/features/save_load_transitions/][document]])           | No (Maybe can =pickle=)                   | No (Maybe can =pickle=. Trained policies can save/load.)   | Yes                        | Yes             |
| *Deep Learning Framework* | Anything                              | TensorFlow 1.14 (only this version)       | Anything (Helper functions for [[https://ray.readthedocs.io/en/latest/rllib-concepts.html#building-policies-in-tensorflow][TensorFlow]] and [[https://ray.readthedocs.io/en/latest/rllib-concepts.html#building-policies-in-pytorch][PyTorch]])     | [[https://chainer.org/][Chainer]] ([[https://chainer.org/announcement/2019/12/05/released-v7.html][maintenance only]]) | TensorFlow 2.3  |


* DONE Misc
CLOSED: [2020-01-17 Fri 22:31]
:PROPERTIES:
:EXPORT_HUGO_SECTION_FRAG: misc
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_WEIGHT: 999
:END:

In this section, cpprb related miscellaneous information are described.

- [[https://ymd_h.gitlab.io/cpprb/misc/links/][Links]]
- [[https://ymd_h.gitlab.io/cpprb/misc/license/][License]]
- [[https://ymd_h.gitlab.io/cpprb/misc/logo/][Logo]]
- [[https://ymd_h.gitlab.io/cpprb/misc/citation/][Citation]]

* DONE Logo
CLOSED: [2020-08-04 Tue 09:19]
:PROPERTIES:
:EXPORT_HUGO_SECTION_FRAG: misc
:EXPORT_FILE_NAME: logo
:END:

[[https://ymd_h.gitlab.io/cpprb/images/logo.png]]

The cpprb logo was designed by the author (H. Yamada) at the beginning
of 2020. The logo comes from the package name "cpprb". The first "c"
letter imitates "replay" button, indicating the key feature of
experience replay. The "r" letter represents reward in formula
(e.g. Bellman equation), which is another key concept of reinforcement
learning.

* DONE FAQ
CLOSED: [2020-06-06 Sat 13:50]
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_SECTION_FRAG: faq
:EXPORT_HUGO_WEIGHT: 900
:END:

#+HTML: {{% notice note %}} <p>
Please see [[https://github.com/ymd-h/cpprb/discussions][discussions]], too.
#+HTML: </p>{{% /notice %}}

** Can I save/load ReplayBuffer?

No. Unfortunately, save/load full ~ReplayBuffer~ object state is not
supported.


However, you can save stored transitions. (See [[https://ymd_h.gitlab.io/cpprb/features/save_load_transitions/][document]])


** "IlIegal instruction (core dumped)" when "import cpprb"

Pre-built binary does not match with your environment. Please
re-install from source code.

#+begin_src shell
pip uninstall cpprb
pip install cpprb --no-binary
#+end_src

This installation requires sufficient C++ compiler. (see [[https://ymd_h.gitlab.io/cpprb/installation/][Installation]])

* DONE Survey
CLOSED: [2021-01-23 Sat 09:18]
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_WEIGHT: 950
:EXPORT_HUGO_SECTION_FRAG: survey
:END:

In this section, we try to survey papers related to experience replay.

Some of them can be implemented in cpprb in future. Some can be just
know-how of experience replay.

If you have any ideas or requests, please post to [[https://github.com/ymd-h/cpprb/discussions][GitHub Disccusions]].


- [[https://ymd_h.gitlab.io/cpprb/survey/ero/][Experience Replay Optimization (ERO)]]
- [[https://ymd_h.gitlab.io/cpprb/survey/ref-er/][Remember and Forget for Experience Replay (ReF-ER)]]
- [[https://ymd_h.gitlab.io/cpprb/survey/aer/][Attentive Experience Replay (AER)]]
- [[https://ymd_h.gitlab.io/cpprb/survey/cer/][Competitive Experience Replay (CER)]]
- [[https://ymd_h.gitlab.io/cpprb/survey/der/][Dynamic Experience Replay (DER)]]
- [[https://ymd_h.gitlab.io/cpprb/survey/ners/][Neural Experience Replay Sampler (NERS)]]
- [[https://ymd_h.gitlab.io/cpprb/survey/combined_er/][Combined Experience Replay (CER)]]
- [[https://ymd_h.gitlab.io/cpprb/survey/lfiw/][Likelihood-free Importance Weights (LFIW)]]
- [[https://ymd_h.gitlab.io/cpprb/survey/parallel/][Parallel Actors and Learners]]
- [[https://ymd_h.gitlab.io/cpprb/survey/maper/][Model-augmented Prioritized Experience Replay (MaPER)]]




* DONE Experience Replay Optimization (ERO)
CLOSED: [2021-01-23 Sat 09:28]
:PROPERTIES:
:EXPORT_HUGO_SECTION_FRAG: survey
:EXPORT_FILE_NAME: ero
:END:

** Overview
[[https://www.ijcai.org/Proceedings/2019/589][Experience Replay Optimization (ERO)]] masks stored transitions in order
to improve sample efficiency. Additional neural network, "replay
policy", takes features extracted from each transition and infers mask
probability. Agent samples from masked transitions uniformly.

In order to train replay policy, binary masks (\( \mathbf{I} \))
produced by Bernoulli distribution are considered as action. The
replay-reward (\( r^{r} \)) as defined as the difference between
culmutive reward of the previous policy (\( \pi ^{\prime} \)) and that
of agent policy (\( \pi \)); \( r ^{r} = r^{c}_{\pi} - r^{c}_{\pi
^{\prime}} \).

The policy gradient for mini batch can be written as follows;

\[ \sum _{j:B_j \in B^{\text{batch}}} r^r \nabla [ \mathbf{I}_j \log \phi + (1-\mathbf{I}_j) \log (1-\phi) ] \]


** With cpprb
We plan to implement somthing like =BernoulliMaskedReplayBuffer=
to support ERO. Together with such future enhancement, users still
need to implement neural network which infers probabilities of
Bernoulli masks.

** References
- [[https://www.ijcai.org/Proceedings/2019/589][D. Zha et al., "Experience Replay Optimization", IJCAI (2019) 4243-4249]] ([[https://arxiv.org/abs/1906.08387][arXiv]])

* DONE Remember and Forget for Experience Replay (ReF-ER)
CLOSED: [2021-01-24 Sun 11:39]
:PROPERTIES:
:EXPORT_HUGO_SECTION_FRAG: survey
:EXPORT_FILE_NAME: ref-er
:END:

** Overview
[[http://proceedings.mlr.press/v97/novati19a.html][Remember and Forget for Experience Replay (ReF-ER)]] dismisses
transtions from "far policy". The key metrics, importance
(\(\rho_t\)), is the ratio between the probability of selecting
(\(a_t\)) with the current policy (\(\pi ^{w}\)) and with the behavior
policy (\(\mu _t\)); \(\rho _t = \pi (a_t\mid s_t) / \mu _t (a_t\mid s_t)\).

If \(1/c_{\text{max}} < \rho _t < c_{\text{max}}\) then it is classified
as "near-policy", otherwise "far-policy". The gradients
(\(\hat{g}(w)\)) computed from far-policy are clipped to 0.

Additionally, penalty term (\(\hat{g}^D(w)\)) is defined as follows;
\(\hat{g}^D(w)=E[\nabla D_{\text{KL}}(\mu _k ( \cdot \mid s_k)\| \pi ^w ( \cdot \mid s_k))]\).

These two terms are added with annealing parameter \(\beta\);

\[ \hat{g}^{\text{ReF-ER}}(w) = \beta \hat{g}(w) + (1-\beta) \hat{g}^D(w) \]

The \(\beta \) is updated at each step by following rule;

\[ \beta \leftarrow \cases{ (1-\eta )\beta & if \(n_{\text{far}} /N > D\) \cr (1-\eta )\beta + \eta & otherwise} \]

where \(\eta\) is learning rate of neural network, \(N\) is the number
of total samples in the replay buffer, \(n_{\text{far}}\) is the
number of far policy samples in the replay buffer, and \(D\) is a
hyperparameter.

** With cpprb
Under investigation. It is still not clear how do the authors sample
from replay buffer. We continue to investigate [[https://github.com/cselab/smarties][their code]].

** References
- [[http://proceedings.mlr.press/v97/novati19a.html][G. Novati and P. Koumoutsakos, "Remember and Forget for Experience Replay", ICML (2019)]] ([[https://arxiv.org/abs/1807.05827][arXiv]], [[https://github.com/cselab/smarties][code]])

* DONE Attentive Experience Replay (AER)
CLOSED: [2021-01-24 Sun 22:59]
:PROPERTIES:
:EXPORT_FILE_NAME: aer
:EXPORT_HUGO_SECTION_FRAG: survey
:END:

** Overview
[[https://ojs.aaai.org//index.php/AAAI/article/view/6049][Attentive Experience Replay (AER)]] uniformly samples \(\lambda\) times
greater size than mini-batch size (\(k\)), then choose top \(k\)
similar samples as minibatch.

The coefficient \(\lambda\) is annealed to 1 during training.

The similarity function \(\mathcal{F}(s_j,s_t)\) is task dependent. In
the paper, the authors used cosine similarity for [[http://www.mujoco.org/][MuJoCo]] and norm of
difference between embedded features for Atari 2600 games.

** With cpprb
You can implement AER with simple =ReplayBuffer= class. Simply you can
sample enlarged batch size then compute similarity.

** References
- [[https://ojs.aaai.org//index.php/AAAI/article/view/6049][P. Sun et al., "Attentive Experience Replay", AAAI (2020) 34, 5900-5907]]

* DONE Competitive Eeperience Replay (CER)
CLOSED: [2021-01-30 Sat 12:55]
:PROPERTIES:
:EXPORT_FILE_NAME: cer
:EXPORT_HUGO_SECTION_FRAG: survey
:END:

** Overview
[[https://iclr.cc/Conferences/2019/Schedule?showEvent=1091][Competitive Experience Replay (CER)]] is a strategy for goal-directed RL
with sparse reward. In CER, a pair of agents, \(\pi _A \) and \(\pi _B\),
are trained simultaneously.

Agent \(\pi _A\) is punished if agent \(\pi _B\) is also visited the
near states, \(|s_A^i - s_B^j| < \delta \). Whereas, agent \(\pi _B\)
gets reward if it visits near the agent \(\pi _A\). Such reward
re-labelling is executed per mini-batch.

Depending on initializing methods, there are two variants of
CER. Independent-CER initializes \(\pi _B\) with the task's initial
distribution. Interact-CER initializes \(\pi _B\) with random
off-policy sample of \(\pi _A\).


The authors pointed out that CER complemented the HER. Together with
HER, CER improves performance.

** With cpprb
Because of [[https://ymd_h.gitlab.io/cpprb/features/flexible_environment/][flexible environment design]], cpprb can store a pair of
transitions from 2 agents. However, the current version of cpprb
doesn't have the functionality of sampling episodes instead of
transitions.


** References
- [[https://iclr.cc/Conferences/2019/Schedule?showEvent=1091][H. Liu et al., "Competitive Experience Replay", ICLR 2019]] ([[https://arxiv.org/abs/1902.00528][arXiv]])

* DONE Dynamic Experience Replay (DER)
CLOSED: [2021-02-01 Mon 08:25]
:PROPERTIES:
:EXPORT_HUGO_SECTION_FRAG: survey
:EXPORT_FILE_NAME: der
:END:

** Overview
In [[http://proceedings.mlr.press/v100/luo20a.html][Dynamic Experience Replay (DER)]], not only human demonstrations but
also successful episodes can be used as demonstrations. For robot
control task, human demonstration is not always perfect demonstration,
so that DER auguments and replaces demonstrations by successful
episodes. DER can work with distributed RL framework such [[https://arxiv.org/abs/1803.00933][Ape-X]].


DER uses multiple (global) replay buffers
\(\lbrace\mathbb{B}_0\dots\mathbb{B}_n\rbrace\) which have
demonstration zone. On the episode end, explorers randomly picked one
of the replay buffers \(\mathbb{B}_i\) and stores
transitions. Additionally, if the episode succeeds, it is stored at
another (ordinary) replay buffer \(\mathbb{T}\) for successful
transitions.

Learner picks one of the replay buffers \(\mathbb{B}_j\) randomly,
then samples mini-batch, trains network, and updates priorities as
usual prioritized experience replay. Periodically, demonstration zones
are replaced by randomly sampled transitions from successful
transition replay buffer \(\mathbb{T}\).



** With cpprb
DER requires specialized demonstration zone where transitions are
keeped. Naively speaking, it can be realized by using 2 replay
buffers. However, to sample from this joint replay buffer with
prioritized way, cpprb need some modification. We might implement
something like =JointPrioritizedReplayBuffer=.


** References
- [[http://proceedings.mlr.press/v100/luo20a.html][J. Luo and H. Li, "Dynamic Experience Replay", CoRL (2020)]] ([[https://arxiv.org/abs/2003.02372][arXiv]], [[https://sites.google.com/site/dynamicexperiencereplay][site]])

* DONE Neural Experience Experience Replay Sampler (NERS)
CLOSED: [2021-02-13 Sat 11:03]
:PROPERTIES:
:EXPORT_FILE_NAME: ners
:EXPORT_HUGO_SECTION_FRAG: survey
:END:

** Overview
[[https://openreview.net/forum?id=gJYlaqL8i8][Neural Experience Replay Sampler (NERS)]] is neural network learns
sampling score \(\sigma _i\) for each transition. RL policy is trained
like PER, except using inferred score \(\sigma _i\) instead of
\(\mid\text{TD}\mid\).


\[ p_i = \frac{(\sigma _i) ^{\alpha}}{\sum _{k \in [\mid B\mid ]} (\sigma _k) ^{\alpha}} \]


NERS consists of 3 neural networks, local network \(f_l\), global
network \(f_g\), and score network \(f_s\). \(f_l\) calculates for
each transisiton. \(f_g\) calculates for each transition then these
outputs are averaged over mini-batch as global context.\(f_s\) takes
the concatenation of the outputs of \(f_l\) and the global context and
calculates scores \(\sigma _i\) for each transition.


The inputs of NERS are following set of features from transitions;

\[D(I) = \lbrace s _{\kappa(i)}, a _{\kappa(i)}, r _{\kappa(i)}, s _{\kappa(i)+1}, \kappa(i), \delta _{\kappa(i)}, r_{\kappa(i)} + \gamma \max _a Q _{\hat{\theta}} (s_{\kappa(i)},a) \rbrace _{i \in I} \]

where \(\kappa(i)\) is \(i\)-th time step, \(\gamma\) is discount
factor, \(\delta _{\kappa(i)}\) is TD error, and \(\hat{\theta}\) is
target network parameter.


NERS are trained at each episode end with subset transitions of that
used for updating of actor and critic during the episode.

In order to optimize replay reward
\(r ^{\text{re}} = \sum _{t \in \text{current episode}} r_t - \sum _{t \in \text{previous episode}} r_t \),
the following gradients are used;

\[\nabla _{\phi} \mathbb{E} [r^{\text{re}}] = \mathbb{E}[r^{\text{re}}\sum _{i\in I_{\text{train}}} \nabla _{\phi} \log \sigma _i (D(I_{\text{i}}))] \]


** With cpprb
You can implement NERS with the current cpprb. For main replay buffer,
you can use =PrioritizedReplayBuffer=. For index collection, you can
use =ReplayBuffer=. Additionally you need to implement neural network
to learn sampling probability.


** References
- [[https://openreview.net/forum?id=gJYlaqL8i8][Y. Oh et al., "Learning to Sample with Local and Global Contexts in Experience Replay Buffer", ICRL (2021)]] ([[https://arxiv.org/abs/2007.07358][arXiv]])

* DONE Combined Experience Replay (CER)
CLOSED: [2021-09-19 Sun 11:37]
:PROPERTIES:
:EXPORT_HUGO_SECTION_FRAG: survey
:EXPORT_FILE_NAME: combined_er
:END:

** Overview
S. Zhang and R. Sutton discussed the effect of replay buffer
size[fn:13]. After the famous DQN paper[fn:11], almost all experiments
fixed the buffer size to \(10^6\). Their experiments indicated that
larger buffer made learning slower. Combined Experience Replay (CER)
is a method where a latest transition is mixed with transitions from
replay buffer. CER improves learning speed, especially for large
replay buffer.

** With cpprb
You can sample =(batch_size - 1)= transitions from replay buffer and
add the latest transition to the batch.


* DONE Likelihood-free Importance Weights (LFIW)
CLOSED: [2021-09-22 Wed 08:26]
:PROPERTIES:
:EXPORT_FILE_NAME: lfiw
:EXPORT_HUGO_SECTION_FRAG: survey
:END:

** Overview
S. Shinha /et al/.[fn:14] proposed Likelihood-free Importance Weights
(LFIW) to compensate distribution shift between transitions taken from
the current policy and those in the replay buffer (taken from many
behavior policies).

The concept is similar to V-trace[fn:15] and Retrace[fn:16], however,
LFIW doesn't require episodic treatment and estimates probability
ratio \(d^{\pi}/d^{\mathcal{D}}\) by using extra network
\(w_{\psi}(s,a)\). First of all, the current distribution \(d^{\pi}\)
is approximated by that in the secondary small (fast) replay buffer
\(d^{\mathcal{D}_f}\).

The estimation of the probability ratio is based on the following
Lemma;

\[ D(P\|Q) \geq \mathbb{E}_P[f^{\prime}(w(x))] - \mathbb{E}_Q[f^{\ast}(f^{\prime}(w(x)))] \text{,}\]

where \(D(\cdot\|\cdot)\) is f-divergence, \( f \) is any convex
lower-semicontinuous function, \(f^{\prime}\) is the first order
derivative, and \(f^{\ast}\) is the convex conjugate. The equality is
achieved when \(w=dP/dQ\).

By minimizing
\(L_{w}(\psi)=\mathbb{E}_{\mathcal{D}}[f^{\ast}(f^{\prime}(w_{\psi}(s,a)))]-\mathbb{E}_{\mathcal{D}_f}[f^{\prime}(w_{\psi}(s,a))]\),
the network \(w_{\psi}(s,a)\) approches to
\(d^{\mathcal{D}_f}/d^{\mathcal{D}}\).

Additionally, to stabilize network update, \(w_{\psi}(s,a)\) is
normalized over the main (slow) replay buffer with temperature
hyperparameter \(T\);

\[\tilde{w}_{\psi}(s,a) = \frac{w_{\psi}(s,a)^{1/T}}{\mathbb{E}_{\mathcal{D}}[w_{\psi}(s,a)^{1/T}]}\]


Unfortunately, this paper was [[https://openreview.net/forum?id=ioXEbG_Sf-a][rejected at ICLR 2021]]. We hope an
improved paper will come up soon.

** With cpprb
You can implement LFIW with cpprb. Two ~ReplayBuffer~ with different
sizes are required.

* DONE Parallel Actors and Learners
CLOSED: [2021-12-30 Thu 16:35]
:PROPERTIES:
:EXPORT_HUGO_SECTION_FRAG: survey
:EXPORT_FILE_NAME: parallel
:END:

** Overview
C. Zhang /et al/.[fn:19] proposed a framework for generating scalable
RL which consists of parallel actors and parallel learners.

One of the major differences is implementing Segment Tree for PER with
K-ary tree instead of ordinary binary tree. According to the authors,
K-ary tree reduces the depth of intermediate levels, which speeds up
priority update during insertion. Moreover, K-ary tree reduces total
memory size, and reduces the number of cache misses. (To be honest, 1
million size segment tree can be loaded on L2 cache of their CPU, and
it is not so critical as the authors had considered.)

In order to allow parallel read and write transitions, the authors
proposed "lazy writing", which first sets priorities to 0 to avoid
sampling, then updates transition data, and finally sets true
priorities. This approach achieves lockless parallelism for transition
data (not for the segment tree).

The authors divide segment tree lock into global lock and last level
lock in order to read priorities during intermediate update.

Additionally, their program can decide the number of actors and
learners from the hardware configuration and the desired throughput.

** Comparison with cpprb
As far as we know, they don't publish their source code. Once they
disclose it, we would like to compare it with our implementation.

We try to summarize similarities and differences between their framework and our ~MPPrioritizedReplayBuffer~.

|                                               | The Paper's Framework                                                 | ~cpprb.MPPrioritizedReplayBuffer~                                                                                                                                                                                                                                                                             |
|-----------------------------------------------+-----------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Multiple Actors                               | Yes                                                                   | Yes                                                                                                                                                                                                                                                                                                           |
| Multiple Learners                             | Yes. Gradients are summarized at parameter server.                    | No                                                                                                                                                                                                                                                                                                            |
| Segment Tree                                  | K-ary Tree on 1-d Contiguous Array. Propagate only diff value[fn:20]. | Binary Tree on 1-d Contiguous Array. Propagate value itself.                                                                                                                                                                                                                                                  |
| Priority Update                               | Write the last level and update intermediate levels with lock.        | Write the last level parallelly. Update intermediate levels with lock just before ~sample~ to avoid inconsistency.                                                                                                                                                                                                  |
| Lock                                          | Segment Tree Global and Last Level Locks. (and Atomic Index?)         | Atomic Index, Transition Data Lock and Segment Tree Lock. (See [[https://ymd_h.gitlab.io/cpprb/features/ape-x/][Ape-X]])                                                                                                                                                                                                                                         |
| Lock Architecture                             | (Maybe) Ordinary Exclusive Lock                                       | Parallel writing is allowed (because writing at different address) and the number of writers in the critical section is atomically traced. Reading has higher priority and prevents writers from entering the critical section again. Reader starts working after all writers exit from the critical section. |
| Inconsistency of Sampling and Priority Update | Allow weight inconsistence because of little impact in practice       | Atomically extract weights, too. (Always consistent)                                                                                                                                                                                                                                                          |
| Implementation                                | Wrtten in C++. There is Python binding to plugin into [[https://github.com/ray-project/ray/tree/master/rllib][RLlib]][fn:21]    | Written in C++ (Segment Tree) and Cython (Buffer). Provided as Python class.                                                                                                                                                                                                                                  |

* DONE Model-augmented Prioritized Experience Replay (MaPER)
CLOSED: [2022-02-09 Wed 07:22]
:PROPERTIES:
:EXPORT_HUGO_SECTION_FRAG: survey
:EXPORT_FILE_NAME: maper
:END:

** Overview
Model-augmented Prioritized Experience Replay (MaPER), which was
proposed by Y. Oh /et al/.[fn:23], extends critic network in order to
predict Q-value better. The critic network, Model-augumented Critic
Network (MaCN), predicts not only Q-value but also reward and next
state with shared weights.

In MaPER, the combination of TD error and model prediction errors is
considered as transition priority
$\sigma_i = \xi_Q |\delta Q_{\theta}|_{MSE} + \xi_R |\delta R_{\theta}|_{MSE} + \xi_S |\delta S_{\theta}|_{MSE}$.

The coefficients are adaptively changed with following rule;

\[ \xi_j = \frac{1}{Z}\exp \left ( \frac{\mathcal{L}_j^{t-1}}{\mathcal{L}_j^{t-2}T}\right )~\text{where}~j=Q,R,S \]

MaPER works as a kind of curriculum learning. It starts learning from
transitions with high prediction errors. Then, after learning model
dynamism, transitions with large TD error are used. According to their
experiments, TD error decreases faster and estimated Q values are
match to the returns (aka. episode rewards) well.

** With cpprb
You can implement MaPER with ~PrioritizedReplayBuffer~.

* DONE Understanding of Experience Replay
CLOSED: [2021-02-13 Sat 16:28]
:PROPERTIES:
:EXPORT_HUGO_SECTION_FRAG: understanding
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_WEIGHT: 925
:END:

** Basics of Experience Replay
Experience Replay is widely used for off-policy reinforcement
learning. Naively speaking, Experience Replay is the method where
transitions are stored at so called Replay Buffer (or Replay Memory)
once, then these transitions are "randomly" sampled as mini-batch for
training. One of the largest motivation is sample efficiency. Running
an environment, especially in the real world, is costful, so that we
want to reuse already sampled transitions as much as
possible. Moreover, many results shows that Experience Replay speeds
up training.

Experience Replay was proposed (or at leaset was named) by
L. Lin[fn:2]. At the same paper, he also noted that replaying past
experiences would disturb if the past policies were different from the
current one. The policy might overestimate or underestimate by
learning already rare action results. (Precisely speaking, tabular
version of Q-learning is not affected.)

There are a lot of studies and modifications of Experience
Replay. Most of them focus the way of selecting transitions in order
to improve sample efficiency.

** On-Policyness Effect at Experience Replay
W. Fedus /et.al./[fn:4] recently studied the effect of on-policyness
(closeness between the current policy and behavior policy). The key
metrics at this paper is the age of transition, which is defined by
the number of gradient steps taken by the learner since the transition
was generated.

Usually, Replay Buffer is implemented with a fixed size ring buffer,
where the oldest transition is overwritten by a newly coming
transition when the capacity is already full. Therefore, replay ratio
(\(=\frac{\text{gradient update}}{\text{environment transition}}\)) is
a good factor to adjust the oldest age of transition stored in the
Replay Buffer.

Their results shows that performance improves when increasing replay
buffer capacity while fixing the oldest age of transition, which also
requires decrease of replay ratio. Performance also improves when
reducing the oldest age of transition while fixing replay buffer
capacity, which also requires decrease of replay ratio.

If the capacity is increased without changing replay ratio, the
results vary depending on the experimental setup. There are
competitive 2 effects, improvement by increasing the capacity and
deterioration by the older age of transition.


The authors also found \(n\)-step rewards enlarges the amount of
improvement with larger replay buffer capacity even though \(n\)-step
rewards with the old age of transition were not theoretically
guaranteed.


** Prioritized Experience Replay
One of the most well-known extension of Exparience Replay is
Prioritized Experience Replay (PER) proposed by T. Schaul
/et al./[fn:3]

The key concept of PER is utilizing TD error (\( \delta _i \)) for
prioritization (\(p_i\)). There are 2 variants of PER,
proportional-base \(p_i=|\delta_i|+\epsilon\) and rank-base
\(p_i=\frac{1}{\text{rank}(i)}\), where \(\epsilon\) is a small
positive value to avoid zero prioritization and \(\text{rank}(i)\) is
rank of transition \(i\) when transitions are sorted according to
\(|\delta_i|\). Theoretically, rank-base is more robust because it is
insensitive to outliers, however, experimental results are not so much
different and proportional-base has efficient implementation, so that
many libraries including cpprb adopt proportional-base
one[fn:5][fn:6][fn:7].

The probability of sampling transition \(i\) is following;

\[ P(i) = \frac{(p _i)^{\alpha}}{\sum _k (p_k)^{\alpha}} \]

where \(\alpha\) is a hyperparameter of prioritization. If
\(\alpha=0\), no prioritization are applied.

In order to correct distribution, following importance sampling
weights are used;

\[ w_i = \left ( \frac{1}{N}\frac{1}{P(i)} \right )^{\beta} \]

where \(N\) is the number of transitions in the Replay
Buffer. \(\beta\) is a hyperparameter of compensation. Weighted TD
error \(w_i\delta_i\) are used instead of \(\delta_i\).


** Theory of PER Success
A. Ang /et al./[fn:8] tried to explain the success of PER from
theoretical view point. (Unfortunately, the paper itself was rejected
by ICLR 2021. I hope to see the improved version in future.)

The authors utilized Expected Value of Backup (EVB), which is defined
as improvement of cumulative discounted rewards by updating on an
experience \(e _k\) (\(=\lbrace s_k, a_k, r_k, s_{k+1}\rbrace\));

\[ \text{EVB}(e_k) = v_{\pi _{\text{new}}} (s_k) - v_{\pi _{\text{old}}} (s_k) = \sum _a \pi _{\text{new}} (a\mid s_k) q_{\pi _{\text{new}}}(s_k,a) - \sum _a \pi _{\text{old}} (a\mid s_k) q_{\pi _{\text{old}}}(s_k,a)\]

where \(\pi _{\text{old}}\), \(v _{\text{old}}\), and
\(q_{\text{old}}\) are the policy, value function, and Q-function
before the update, respectively, and \(\pi _{\text{new}}\),
\(v_{\text{new}}\), and \(q_{\text{new}}\) are those after.

The key result of this paper is that as long as policy improvement is
greedy, the size of EVB is upper-bounded by scaled TD error size;

\[|\text{EVB}(e_k) | \leq \alpha | \text{TD}(s_k, a_k, r_k, s_{k+1})| \]

where \(\alpha\) is step size parameter.

This is not sufficient but partially explains why choosing experience
with large TD error is better. (At least it is true that an experience
with small TD error cannot improve cumulative rewards so much.)


** Relation between Loss Functions and Non-Uniform Sampling
S. Fujimoto /et.al./[fn:9] clarified relation between loss functions
and non-uniform sampling in Experience Replay.

The autors formulated the equivalent loss function for any non-uniform
sampling in terms of expectation;

\[ \mathbb{E}_{\mathcal{D}_1}[\nabla _Q \mathcal{L}_1 (\delta (i))] = \mathbb{E}_{\mathcal{D}_2}\left [\frac{p_{\mathcal{D}_1(i)}}{p_{\mathcal{D}_2}(i)}\nabla _Q \mathcal{L}_1 (\delta (i))\right ] = \mathbb{E}_{\mathcal{D}_2}[\nabla _Q \mathcal{L}_2 (\delta (i))] \]

where \(\nabla _Q \mathcal{L}_2 (\delta (i)) = \frac{p_{\mathcal{D}_1(i)}}{p_{\mathcal{D}_2}(i)}\nabla _Q \mathcal{L}_1 (\delta (i)) \)

The equivalent loss function at uniform sampling to a loss of
\(\frac{1}{\tau}|\delta(i)|^{\tau}\) at PER is following;

\[ \mathcal{L}^{\tau}_{\text{PER}}(\delta(i)) = \frac{\eta N}{\tau + \alpha - \alpha\beta}|\delta(i)|^{\tau + \alpha - \alpha\beta},~\eta = \frac{\min _j | \delta(i) |^{\alpha\beta}}{\sum _j |\delta(i)|^{\alpha}} \]

By minimizing mean squared error (MSE) of TD error, Q-function is
optimized to mean of target
\(y(i)=r+\gamma\mathbb{E}_{s^{\prime},a^{\prime}}[Q(s^{\prime},a^{\prime})]\). By
minimizing L1 loss of TD error, Q-function is optimized to median of
target. If the objective loss is intermediate of MSE and L1
(\(\frac{1}{\tau}|\delta(i)|^{\tau}, \tau\in(1,2)\)), the result
is somehow mixed of the above.

If we use the MSE in PER sampling, \(\tau + \alpha - \alpha\beta > 2\)
and the result is biased. Additionally, the authors pointed out that
the compensation hyperparameter \(\beta\) does not affect expected
gradient result, and \(\beta=0\) was fine.

The authors proposed Loss-Adjusted Prioritized Experience Relpay (LAP) as follows;

\[ p(i) = \frac{\max(|\delta(i)|^{\alpha},1)}{\sum _j \max(|\delta(j)|^{\alpha},1)},~\mathcal{L}_{\text{Huber}}(\delta(i)) = \begin{cases} 0.5\delta(i)^2 & \text{if}~|\delta(i)| \leq 1 \cr |\delta(i)| & \text{otherwise} \end{cases} \]

MSE is used at \(|\delta(i)|\leq 1\) because of smooth gradient
around 0. By using \(\max(|\delta(i)|^{\alpha},1)\), transitions
with \(|\delta(i)|\leq 1\) are sampled uniformly, so that no-bias is
introduced.

The authors also proposed the equivalent form at uniform sampling,
Prioritized Approximation Loss (PAL) as follows;

\[ \mathcal{L}_{\text{PAL}}(\delta(i)) = \frac{1}{\lambda} \begin{cases} 0.5\delta(i)^2 & \text{if}~|\delta(i)| \leq 1 \cr \frac{|\delta(i)|^{1+\alpha}}{1+\alpha} & \text{otherwise} \end{cases},~\lambda = \frac{\sum _j \max(|\delta(i)|^{\alpha},1)}{N} \]

Important notice: In terms of expectation, LAP and PAL are equivalent,
however, the variances are different. PAL (and any other
uniform-sample equivalent) has larger variance and smaller
computational cost.


* Footnotes

[fn:23] [[https://openreview.net/forum?id=WuEiafqdy9H][Y. Oh /et al/., "Model-augmented Prioritized Experience Replay", ICLR (2022)]]

[fn:22] [[https://papers.nips.cc/paper/2017/hash/453fadbd8a1a3af50a9df4df899537b5-Abstract.html][M. Andrychowicz /et al/., "Hindsight Experience Replay", NeurIPS (2017)]] ([[https://arxiv.org/abs/1707.01495][arXiv:1707.01495]])

[fn:21] [[https://icml.cc/Conferences/2018/Schedule?showEvent=2116][E. Liang /et al/., "RLlib: Abstractions for Distributed Reinforcement Learning", ICML (2018)]], ([[https://arxiv.org/abs/1712.09381][arXiv]], [[https://github.com/ray-project/ray/tree/master/rllib][code]])

[fn:20] It is not clear how they manage min tree. Do they use only sum tree?

[fn:19] [[https://hipc.org/accepted-papers/][C. Zang /et al/., "Parallel Actors and Learners: A Framework for Generating Scalable RL Implementations", HiPC (2021)]] ([[https://arxiv.org/abs/2110.01101][arXiv]])

[fn:18] E. Rotinov, "Reverse Experience Replay" (2019), ([[https://arxiv.org/abs/1910.08780][arXiv:1910.08780]])

[fn:17] [[https://dblp.org/db/journals/corr/corr2110.html#journals/corr/abs-2110-01528][T. Lahire /et al/., "Large Batch Experience Replay", CoRR (2021)]] ([[https://arxiv.org/abs/2110.01528][arXiv:2110.01528]], [[https://github.com/SuReLI/laber][code]])

[fn:16] [[https://proceedings.neurips.cc/paper/2016/hash/c3992e9a68c5ae12bd18488bc579b30d-Abstract.html][R. Munos /et al/., "Safe and Efficient Off-Policy Reinforcement Learning", NeurIPS (2016)]] ([[https://arxiv.org/abs/1606.02647][arXiv:1606.02647]])

[fn:15] [[http://proceedings.mlr.press/v80/espeholt18a.html][L. Espeholt /et al/., "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures", ICML (2018), PMLR 80:1407-1416]] ([[https://arxiv.org/abs/1802.01561][arXiv:1802.01561]], [[https://github.com/deepmind/scalable_agent][code]])

[fn:14] [[https://arxiv.org/abs/2006.13169][S. Shinha /et al/., "Experience Replay with Likelihood-free Importance Weights", (2020) arXiv:2006.13169]]

[fn:13] [[https://sites.google.com/view/deeprl-symposium-nips2017/home][S. Zhang and R. Sutton, "A Deeper Look at Experience Replay", NIPS (2017)]], ([[https://arxiv.org/abs/1712.01275][arXiv cs.LG 1712.01275]])

[fn:12] [[https://openreview.net/forum?id=H1Dy---0Z][D. Hogan /et al/., "Distributed Prioritized Experience Replay", ICLR (2018)]] ([[https://arxiv.org/abs/1803.00933][arXiv cs.LG 1803.00933]])

[fn:11] [[https://www.nature.com/articles/nature14236][V. Mnhi /et al/., "Human-level control through deep reinforcement learning", Nature 518, 529-533 (2015)]]

[fn:10] [[https://ojs.aaai.org/index.php/AAAI/article/view/10295][H. Hasselt /et al/., "Deep Reinforcement Learning with Double Q-Learning", Proc. AAAI 30, 1 (2016)]] ([[https://arxiv.org/abs/1509.06461][arXiv cs.LG 1509.06461]])

[fn:9] [[https://arxiv.org/abs/2007.06049][S. Fujimoto /et al/., "An Equivalence between Loss Functions and Non-Uniform Sampling in Experience Replay" (2020) arXiv:2007.06049]]

[fn:8] [[https://arxiv.org/abs/2102.03261][A. Ang /et al/., "Revisiting Prioritized Experience Replay: A Value Perspective" (2021) arXiv:2102.03261]]

[fn:7] [[https://deepmind.com/research/open-source/Reverb][A. Cassirer /et al/., "Reverb: An efficient data storage and transport system for ML research", (2020) https://github.com/deepmind/reverb]] ([[https://github.com/deepmind/reverb][code]])

[fn:6] [[http://proceedings.mlr.press/v80/liang18b.html][E. Liang /et al/., "RLlib: Abstractions for Distributed Reinforcement Learning", ICML (2018)]] ([[https://arxiv.org/abs/1712.09381][arXiv]], [[https://github.com/ray-project/ray][code]])

[fn:5] [[https://github.com/openai/baselines][D. Prafulla /et al/., "Open AI Baeslines", (2017) https://github.com/openai/baselines]]

[fn:4] [[http://proceedings.mlr.press/v119/fedus20a.html][W. Fedus /et al/., "Revisiting Fundamentals of Experience Replay", ICML (2020)]] ([[https://arxiv.org/abs/2007.06700][arXiv]])

[fn:3] [[https://arxiv.org/abs/1511.05952][T. Schaul /et al/., "Prioritized Experience Replay", ICLR (2016)]]

[fn:2] [[https://link.springer.com/article/10.1007/BF00992699][L. Lin, "Self-improving reactive agents based on reinforcement learning, planning and teaching", Mach. Learn. 8, 293-321 (1992) https://doi.org/10.1007/BF00992699]]

[fn:1] Updating segment tree for PER is critical section, too. To
avoid data race, ~MPPrioritizedReplayBuffer~ lazily updates segment
tree from learner process just before ~sample~ method.
